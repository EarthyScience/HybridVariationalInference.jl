<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory · HybridVariationalInference.jl</title><meta name="title" content="Theory · HybridVariationalInference.jl"/><meta property="og:title" content="Theory · HybridVariationalInference.jl"/><meta property="twitter:title" content="Theory · HybridVariationalInference.jl"/><meta name="description" content="Documentation for HybridVariationalInference.jl."/><meta property="og:description" content="Documentation for HybridVariationalInference.jl."/><meta property="twitter:description" content="Documentation for HybridVariationalInference.jl."/><meta property="og:url" content="https://EarthyScience.github.io/HybridVariationalInference.jl/explanation/theory_hvi/"/><meta property="twitter:url" content="https://EarthyScience.github.io/HybridVariationalInference.jl/explanation/theory_hvi/"/><link rel="canonical" href="https://EarthyScience.github.io/HybridVariationalInference.jl/explanation/theory_hvi/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">HybridVariationalInference.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../problem/">Problem</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/basic_cpu/">Basic workflow</a></li><li><a class="tocitem" href="../../tutorials/inspect_results/">Inspect results</a></li></ul></li><li><span class="tocitem">How to</span><ul><li><a class="tocitem" href="../../tutorials/lux_gpu/">.. use GPU</a></li><li><a class="tocitem" href="../../tutorials/logden_user/">.. specify log-Likelihood</a></li><li><a class="tocitem" href="../../tutorials/blocks_corr/">.. model independent parameters</a></li><li><a class="tocitem" href="../../tutorials/corr_site_global/">.. model site-global corr</a></li></ul></li><li><span class="tocitem">Explanation</span></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/reference_public/">Public</a></li><li><a class="tocitem" href="../../reference/reference_internal/">Internal</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theory</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/EarthyScience/HybridVariationalInference.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/EarthyScience/HybridVariationalInference.jl/blob/main/docs/src/explanation/theory_hvi.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h1><h2 id="Setup-of-the-Problem"><a class="docs-heading-anchor" href="#Setup-of-the-Problem">Setup of the Problem</a><a id="Setup-of-the-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Setup-of-the-Problem" title="Permalink"></a></h2><p>The hybrid variational inferecne, HVI, infers a parametric approximations of  the posterior density, <span>$q(\theta|y)$</span>, by comparing model outputs to uncertain observations, <span>$y$</span>.  At the same time, the machine learning model, <span>$g$</span>, is fitted,  which predicts a subset, <span>$\phi_M$</span> of the parameters of the approximation <span>$q$</span>.  In the case where it predicts the marginal means of process-model&#39;s parameters, <span>$\theta$</span>, this corresponds to the same machine learning model, <span>$g$</span>,  as in parameter learning without consideration of uncertainty.</p><p>HVI approximates the posterior distribution of process-model parameters as a transformation of a multivariate normally distributed variable <span>$\zeta$</span>: <span>$q(\theta) = T(q(\zeta))$</span>, <span>$q(\zeta) \sim \mathcal{N}(\mu, \Sigma)$</span>. This allows for efficient sampling, ensures a finite nonzero probability across the entire real space, and, at the same, time provides sufficient flexibility.</p><p><img src="../hybrid_variational_setup.png" alt="image info"/></p><p>The optimized parameters, <span>$\phi = (\phi_g, \phi_P, \phi_q)$</span> are the same for each site.  This allows to apply minibatching, which does not require predicting the  full observation vector, <span>$y$</span>, during parameter fitting.</p><p>In order to learn <span>$\phi_g$</span>, the user needs to provide a batch of <span>$i \in \{1 \ldots n_{b}\}$</span> observation <span>$y_i$</span>, their uncertinaty, <span>$y_{unc,i}$</span>, covariates <span>$x_{Mi}$</span> and drivers <span>$x_{Pi}$</span> in each iteration of the optimization. Moreover, for each <span>$i$</span>, HVI needs to draw <span>$n_{MC}$</span> samples parameters <span>$\zeta_i$</span>, transforms and runs the model to compute a prediction for <span>$y_i$</span> and computes <span>$\log p(y_i)$</span> to estimate the expected value occurring in the ELBO (see next section).</p><h2 id="Estimation-using-the-ELBO"><a class="docs-heading-anchor" href="#Estimation-using-the-ELBO">Estimation using the ELBO</a><a id="Estimation-using-the-ELBO-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation-using-the-ELBO" title="Permalink"></a></h2><p>In order to find the parameters of the approximation of the posterior, HVI minimizes the KL divergence between the approximation and the true posterior. This is achieve by maximizing the evidence lower bound (ELBO).</p><p class="math-container">\[\mathcal{L}(\phi) = \mathbb{E}_{q(\theta)} \left[\log p(y,\theta) \right] - \mathbb{E}_{q(\theta)} \left[\log q(\theta) \right]\]</p><p>The second term is the entropy of the approximating distribution, which has a closed form  for a multivariate normal distribution. The expectation of the first term can be estimated using Monte-Carlo integration.  When combined with stochastic gradient descent, this needs only a small number of samples.  However, HVI needs to compute the gradient of this expectation of the joint posterior  density of observations and parameter,  <span>$\log p(y,\theta) = \log p(y|\theta) + \log p(\theta)$</span>,  by automatic differentiation. Hence, HVI needs to differentiate the process-model, <span>$f$</span>,  that is run during computation of the Likelihood of the data, <span>$p(y|\theta)$</span>.</p><h2 id="Parameter-transformations"><a class="docs-heading-anchor" href="#Parameter-transformations">Parameter transformations</a><a id="Parameter-transformations-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-transformations" title="Permalink"></a></h2><p>HVI prescribes <span>$q(\theta)$</span> to be the distribution of a transformed random variable,  <span>$\theta = T^{-1}(\zeta)$</span>, where <span>$\zeta = T(\theta)$</span> has a multivariate Normal distribution  (MVN) in unconstrained <span>$\mathbb{R}^n$</span>. The transformation, <span>$T$</span>, provides more flexibility  to model the posterior and takes care of the case where the support of <span>$q(\theta)$</span> is  smaller than <span>$\mathbb{R}^n$</span>, the support of the MVN. For example if the the log of  <span>$\theta$</span> is normally distributed, then <span>$\theta$</span> has LogNormal distribution, and  <span>$\theta = T^{-1}(\zeta) \equiv e^{\zeta}$</span>. The transformed joint density then is</p><p class="math-container">\[p_\zeta(y,\zeta) = p(x, T^{-1}(\zeta)) \, \left| det J_{T^{-1}}(\zeta)\right|,\]</p><p>where <span>$\left| det J_{T^{-1}}(\zeta)\right|$</span> denotes the absolute value of the determinant of the Jacobian of the inverse of transformation, <span>$T$</span> evaluated at <span>$\zeta$</span>.</p><p>With those assumptions, the ELBO becomes </p><p class="math-container">\[\mathcal{L}(\phi) = \mathbb{E}_{q(\zeta)} \left[ \log p(y, T^{-1}(\zeta)) + \log \left| det J_{T^{-1}}(\zeta)\right|  \right] + \mathbb{H}_{q(\zeta)},\]</p><p>where <span>$\mathbb{H}_{q(\zeta)}$</span> is the entropy of the approximating density and the expectation is across a normally distributed random variable, <span>$\zeta$</span>. </p><h2 id="Covariance-structure"><a class="docs-heading-anchor" href="#Covariance-structure">Covariance structure</a><a id="Covariance-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Covariance-structure" title="Permalink"></a></h2><p>HVI assumes that transforms of the latent variable follow a multivariate normal distribution: <span>$\zeta = T((\theta_P, \theta_M)) = (\zeta_P, \zeta_M) \sim \mathcal{N}(\mu(\phi_g), \Sigma)$</span>. The covariance matrix can be decomposed into standard deviation and the correlation matrix.</p><p class="math-container">\[\Sigma = diag(\sigma_\zeta) C_\zeta \, diag(\sigma_\zeta),\]</p><p>where <span>$\sigma_\zeta$</span> is the vector of standard deviations, and <span>$C$</span> is the correlation matrix. HVI further assumes that uncertainties of site parameters, <span>$\zeta_{M1}, \zeta_{M2}, \ldots$</span>, differ only by their standard deviation, i.e. that the parameter correlations is the same and independent of other sites. With the additional assumption of <span>$\zeta_{Ms}$</span> being independent of <span>$\zeta_P$</span>, the covariance matrix has a block-diagonal structure with one block for <span>$\zeta_P$</span> and <span>$n_{site}$</span> repetitions of a block for <span>$\zeta_{M}$</span>. By definition of a correlation matrix, all the main diagonal elements are 1. E.g. for 2 elements in <span>$\zeta_{P}$</span> and 3 enlements in <span>$\zeta_{M}$</span> this results in: </p><p class="math-container">\[\begin{pmatrix}
\begin{matrix} 1 &amp; \rho_{Pab} \\ \rho_{Pab} &amp; 1 \end{matrix} 
&amp; 0 &amp; 0 &amp; \cdots\\ 
0 &amp; 
\begin{matrix} 1 &amp; \rho_{Mab} &amp; \rho_{Mac} \\ \rho_{Mab} &amp; 1 &amp; \rho_{Mbc} \\ \rho_{Mac} &amp; \rho_{Mbc} &amp; 1 \end{matrix} 
&amp; 0
\\
0 &amp; 0 &amp;
\begin{matrix} 1 &amp; \rho_{Mab} &amp; \rho_{Mac} \\ \rho_{Mab} &amp; 1 &amp; \rho_{Mbc} \\ \rho_{Mac} &amp; \rho_{Mbc} &amp; 1 \end{matrix}
\\
\cdots &amp;  &amp; &amp; \ddots
\end{pmatrix}\]</p><p>In order to draw random numbers from such a normal distribution, the Cholesky  decomposition of the covariance matrix is required: <span>$\Sigma = U_{\Sigma}^T U_{\Sigma} =  diag(\sigma_\zeta)^T U_C^T U_C \, diag(\sigma_\zeta)$</span>, where <span>$U_{\Sigma}$</span> and <span>$U_C$</span> are  the cholesky factors of the covariance and correlation matrices respectively. They are  upper triangular matrices. </p><p>Since, the block-diagonal structure of the correlation matrix carries over to the cholesky  factor, <span>$U_C$</span> is a block-diagonal matrix of smaller cholesky factors. If HVI modeled the  depence between <span>$\zeta_{Ms}$</span> and <span>$\zeta_P$</span>, the correlation matrhix would have an  additional block repeated in the first row and its transpose repeated in the first column  in <span>$\Sigma$</span>, leading to a cholesky factor <span>$U_C$</span> having entries in all the rows. </p><p>HVI allows  to accoung for correlations among those parameters by providing the values of the global parameters to the machine learning model, <span>$g$</span> in addition to the covariates.</p><p class="math-container">\[
p(\zeta_{Ms}, \zeta_P) = p(\zeta_{Ms} | \zeta_P) p(\zeta_P)\]</p><p>Since the differentiation through a general cholesky-decomposition is problematic,  HVI directly parameterizes the Cholesky factor of the correlation matrix rather than the  correlation matrix itself. For details see the Wutzler in prep.</p><h2 id="Combining-variational-inference-(VI)-with-hybrid-models"><a class="docs-heading-anchor" href="#Combining-variational-inference-(VI)-with-hybrid-models">Combining variational inference (VI) with hybrid models</a><a id="Combining-variational-inference-(VI)-with-hybrid-models-1"></a><a class="docs-heading-anchor-permalink" href="#Combining-variational-inference-(VI)-with-hybrid-models" title="Permalink"></a></h2><p>Traditional VI estimates all means and uncertainties of the parameters  <span>$(\zeta_P, \zeta_{M1}, \ldots, \zeta_{Mn} )$</span> by inverting the model given the observations  and its uncertainty. HVI, directly inverts only the means of <span>$\zeta_P$</span>  and predicts the means, <span>$\mu_{\zeta_{Ms}}$</span> of the covariate-dependent parameters  by the machine learning model  <span>$q(X_M, \zeta_P; \phi_q)$</span>.  If there is enough information in the observations, the ML model could predict additional  parameters of the posterior distribution based on covariates, such as diagonals of the  covariance matrix. </p><p>Currently, HVI assumes  <span>$\zeta_{Ms} \sim \mathcal{N}(\mu_{\zeta_{Ms}}, \Sigma(\mu_{\zeta_{Ms}}))$</span>  is normally distributed with the covariance matrix <span>$\Sigma$</span> being only dependent on the  magnitude of <span>$\mu_{\zeta_{Ms}}$</span>, i.e. conditionally independent of covariates, <span>$X_M$</span> (see details on <span>$\mu_{\zeta_{Ms}}$</span>).</p><p>In the specific setting, the parameter vector to be opmized,  <span>$\phi = (\phi_P, \phi_g, \phi_u)$</span>, comprises </p><ul><li><span>$\phi_P = \mu_{\zeta_P}$</span>: the  means of the distributions of the transformed global    parameters, </li><li><span>$\phi_g$</span>: the parameters of the machine learning model, and </li><li><span>$\phi_u$</span>: parameterization of <span>$\Sigma_\zeta$</span> that is additional to the means. </li></ul><h3 id="Details"><a class="docs-heading-anchor" href="#Details">Details</a><a id="Details-1"></a><a class="docs-heading-anchor-permalink" href="#Details" title="Permalink"></a></h3><p>Specifically, <span>$\phi_u= (log\sigma^2_P, log\sigma^2_{M0}, log\sigma^2_{M\eta}, a_P, a_M)$</span>,  where the variance of <span>$\zeta_P$</span> is <span>$\sigma^2_P$</span>, the variance of the <span>$i^{th}$</span> entry of  <span>$\zeta_{M}$</span> scales with its magnitude:  <span>$\log \sigma^2_{Mi} = log\sigma^2_{M0i} + log\sigma^2_{M\eta i} \, \mu_{\zeta_{Mi}}$</span>,  and <span>$a_P$</span> and <span>$a_M$</span> are parameter vectors of the blocks of the correlation matrix.</p><p>In order to account for correlations between global and site-specific parameters, HVI models <span>$p(\zeta)$</span> as a multivariate normal distribution that is a shifted  zero-zentered multivariate normal, <span>$p(\zeta_r)$</span>. </p><p class="math-container">\[\zeta = (\zeta_P, \zeta_{Ms}) = \zeta_r + (\mu_{\zeta_P}, \mu_{\zeta_{Ms}})  
\\
\zeta_r = (\zeta_{rP}, \zeta_{rMs})  \sim \mathcal{N}(0, diag(\sigma_\zeta)^T C_\zeta \, diag(\sigma_\zeta))
\\ 
\mu_{\zeta_{Ms}} = g_s(X_M, \zeta_P; \phi_g)
\\
diag(\sigma_\zeta) = e^{(\log\sigma^2_P, \log\sigma^2_{M})/2}
\\
C_\zeta = U^T U
\\
U = \operatorname{BlockDiag}(a_P, a_M)\]</p><p>where the predicted value of <span>$\mu_{\zeta_{Ms}}$</span> may depend on the random draw value of <span>$\zeta_P = \zeta_{r,P} + \mu_{\zeta_P}$</span>. By this construction HVI better supports the assumption that <span>$\zeta_{rM}$</span> is conditionally independent of <span>$\zeta_{rP}$</span>, which is required to macke the cholesky-factor, <span>$U$</span> of the covariance matrix block-diagonal.</p><p>The above procedure makes an additional subtle approximation. HVI allows the variance of <span>$\zeta_{M}$</span> to scale with its magnitude. In the computation of the correlation matrix, however, HVI uses the mean, <span>$\mu_{\zeta_{Mi}}$</span>, rather than the actual sampled value, <span>$\zeta_{Mi}$</span>. If it used the actual value, then the distribution of <span>$\zeta$</span> would need to be described as a general distribution, <span>$p(\zeta) = p(\zeta_{Ms}|\zeta_P) \, p(\zeta_P)$</span>, that would not  be normal any more, and HVI could not compute the expectation by drawing centered normla random numbers.</p><h4 id="Implementation-of-the-cost-function"><a class="docs-heading-anchor" href="#Implementation-of-the-cost-function">Implementation of the cost function</a><a id="Implementation-of-the-cost-function-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-of-the-cost-function" title="Permalink"></a></h4><p>In practical terms the cost function </p><ul><li>generates normally distributed random values <span>$(\zeta_{rP}, \zeta_{rMs})$</span> based on the cholesky factor of the covariance matrix, which depends on optimized parameters <span>$(a_P, a_M)$</span></li><li>generates a sample of <span>$\zeta_P$</span> by adding optimized parameters <span>$\mu_{\zeta_P}$</span> to <span>$\zeta_{rP}$</span></li><li>computes expected value of <span>$\mu_{\zeta_{Ms}}$</span> using the machine learning model given covariates, <span>$X_M$</span>, given <span>$\zeta_P$</span>, and given optimized parameters <span>$\phi_g$</span>.</li><li>generates a sample of <span>$\zeta_{Ms}$</span> by adding the computed <span>$\mu_{\zeta_{Ms}}$</span> to <span>$\zeta_{rMs}$</span></li><li>transforms <span>$(\zeta_{P}, \zeta_{Ms})$</span> to the original scale to get a sample of model parameters <span>$(\theta_{rP}, \theta_{rMs})$</span></li><li>computes negative Log-density of observations for each sample using the physical model, <span>$f$</span>, and subtract the absolute determinant of the transformation, evaluated at the sample.</li><li>approximates the expected value of the former by taking the mean across the samples</li><li>subtract the entropy of the normal distribution approximator </li></ul><p>The automatic differentiation through this cost function including calls to <span>$g$</span>, T, and <span>$f$</span> allows to estimate parameters, <span>$\phi$</span>, by a stochastic gradient decent method.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 13 February 2026 15:41">Friday 13 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
