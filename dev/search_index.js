var documenterSearchIndex = {"docs":
[{"location":"explanation/theory_hvi/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"explanation/theory_hvi/#Setup-of-the-Problem","page":"Theory","title":"Setup of the Problem","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"The hybrid variational inferecne, HVI, infers a parametric approximations of  the posterior density, q(thetay), by comparing model outputs to uncertain observations, y.  At the same time, the machine learning model, g, is fitted,  which predicts a subset, phi_M of the parameters of the approximation q.  In the case where it predicts the marginal means of process-model's parameters, theta, this corresponds to the same machine learning model, g,  as in parameter learning without consideration of uncertainty.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"HVI approximates the posterior distribution of process-model parameters as a transformation of a multivariate normally distributed variable zeta: q(theta) = T(q(zeta)), q(zeta) sim mathcalN(mu Sigma). This allows for efficient sampling, ensures a finite nonzero probability across the entire real space, and, at the same, time provides sufficient flexibility.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"(Image: image info)","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"The optimized parameters, phi = (phi_g phi_P phi_q) are the same for each site.  This allows to apply minibatching, which does not require predicting the  full observation vector, y, during parameter fitting.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In order to learn phi_g, the user needs to provide a batch of i in 1 ldots n_b observation y_i, their uncertinaty, y_unci, covariates x_Mi and drivers x_Pi in each iteration of the optimization. Moreover, for each i, HVI needs to draw n_MC samples parameters zeta_i, transforms and runs the model to compute a prediction for y_i and computes log p(y_i) to estimate the expected value occurring in the ELBO (see next section).","category":"page"},{"location":"explanation/theory_hvi/#Estimation-using-the-ELBO","page":"Theory","title":"Estimation using the ELBO","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In order to find the parameters of the approximation of the posterior, HVI minizes the KL divergence between the approximation and the true posterior. This is achieve by maximizing the evidence lower bound (ELBO).","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"mathcalL(phi) = mathbbE_q(theta) leftlog p(ytheta) right - mathbbE_q(theta) leftlog q(theta) right","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"The second term is the entropy of the approximating distribution, which has a closed form  for a multivariate normal distribution. The expectation of the first term can be estimated using Monte-Carlo integration.  When combined with stochastic gradient descent, this needs only a small number of samples.  However, HVI needs to compute the gradient of this expectation of the joint posterior  density of observations and parameter,  log p(ytheta) = log p(ytheta) + log p(theta),  by automatic differentiation. Hence, HVI needs to differentiate the process-model, f,  that is run during computation of the Likelihood of the data, p(ytheta).","category":"page"},{"location":"explanation/theory_hvi/#Parameter-transformations","page":"Theory","title":"Parameter transformations","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"HVI prescribes q(theta) to be the distribution of a transformed random variable,  theta = T^-1(zeta), where zeta = T(theta) has a multivariate Normal distribution  (MVN) in unconstrained mathbbR^n. The transformation, T, provides more flexibility  to model the posterior and takes care of the case where the support of q(theta) is  smaller than mathbbR^n, the support of the MVN. For example if the the log of  theta is normally distributed, then theta has LogNormal distribution, and  theta = T^-1(zeta) equiv e^zeta. The transformed joint density then is","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"p_zeta(yzeta) = p(x T^-1(zeta))  left det J_T^-1(zeta)right","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"where left det J_T^-1(zeta)right denotes the absolute value of the determinant of the Jacobian of the inverse of transformation, T evaluated at zeta.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"With those assumptions, the ELBO becomes ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"mathcalL(phi) = mathbbE_q(zeta) left log p(y T^-1(zeta)) + log left det J_T^-1(zeta)right  right + mathbbH_q(zeta)","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"where mathbbH_q(zeta) is the entropy of the approximating density and the expectation is across a normally distributed random variable, zeta. ","category":"page"},{"location":"explanation/theory_hvi/#Covariance-structure","page":"Theory","title":"Covariance structure","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"HVI assumes that transforms of the latent variable follow a multivariate normal distribution: zeta = T((theta_P theta_M)) = (zeta_P zeta_M) sim mathcalN(mu(phi_g) Sigma). The covariance matrix can be decomposed into standard deviation and the correlation matrix.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Sigma = diag(sigma_zeta) C_zeta  diag(sigma_zeta)","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"where sigma_zeta is the vector of standard deviations, and C is the correlation matrix. HVI further assumes that uncertainties of site parameters, zeta_M1 zeta_M2 ldots, differ only by their standard deviation, i.e. that the parameter correlations is the same and independent of other sites. With the additional assumption of zeta_Ms being independent of zeta_P, the covariance matrix has a block-diagonal structure with one block for zeta_P and n_site repetitions of a block for zeta_M. By definition of a correlation matrix, all the main diagonal elements are 1. E.g. for 2 elements in zeta_P and 3 enlements in zeta_M this results in: ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"beginpmatrix\nbeginmatrix 1  rho_Pab  rho_Pab  1 endmatrix \n 0  0  cdots \n0  \nbeginmatrix 1  rho_Mab  rho_Mac  rho_Mab  1  rho_Mbc  rho_Mac  rho_Mbc  1 endmatrix \n 0\n\n0  0 \nbeginmatrix 1  rho_Mab  rho_Mac  rho_Mab  1  rho_Mbc  rho_Mac  rho_Mbc  1 endmatrix\n\ncdots     ddots\nendpmatrix","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In order to draw random numbers from such a normal distribution, the Cholesky  decomposition of the covariance matrix is required: Sigma = U_Sigma^T U_Sigma =  diag(sigma_zeta)^T U_C^T U_C  diag(sigma_zeta), where U_Sigma and U_C are  the cholesky factors of the covariance and correlation matrices respectively. They are  upper triangular matrices. ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Since, the block-diagonal structure of the correlation matrix carries over to the cholesky  factor, U_C is a block-diagonal matrix of smaller cholesky factors. If HVI modeled the  depence between zeta_Ms and zeta_P, the correlation matrhix would have an  additional block repeated in the first row and its transpose repeated in the first column  in Sigma, leading to a cholesky factor U_C having entries in all the rows. ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"HVI allows  to accoung for correlations among those parameters by providing the values of the global parameters to the machine learning model, g in addition to the covariates.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"\np(zeta_Ms zeta_P) = p(zeta_Ms  zeta_P) p(zeta_P)","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Since the differentiation through a general cholesky-decomposition is problematic,  HVI directly parameterizes the Cholesky factor of the correlation matrix rather than the  correlation matrix itself. For details see the Wutzler in prep.","category":"page"},{"location":"explanation/theory_hvi/#Combining-variational-inference-(VI)-with-hybrid-models","page":"Theory","title":"Combining variational inference (VI) with hybrid models","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Traditional VI estimates all means and uncertainties of the parameters  (zeta_P zeta_M1 ldots zeta_Mn ) by inverting the model given the observations  and its uncertainty. HVI, directly inverts only the means of zeta_P  and predicts the means, mu_zeta_Ms of the covariate-dependent parameters  by the machine learning model  q(X_M zeta_P phi_q).  If there is enough information in the observations, the ML model could predict additional  parameters of the posterior distribution based on covariates, such as diagonals of the  covariance matrix. ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Currently, HVI assumes  zeta_Ms sim mathcalN(mu_zeta_Ms Sigma(mu_zeta_Ms))  is normally distributed with the covariance matrix Sigma being only dependent on the  magnitude of mu_zeta_Ms, i.e. conditionally independent of covariates, X_M (see details on mu_zeta_Ms).","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In the specific setting, the parameter vector to be opmized,  phi = (phi_P phi_g phi_u), comprises ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"phi_P = mu_zeta_P: the  means of the distributions of the transformed global    parameters, \nphi_g: the parameters of the machine learning model, and \nphi_u: paramerization of Sigma_zeta that is additional to the means. ","category":"page"},{"location":"explanation/theory_hvi/#Details","page":"Theory","title":"Details","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"Specifically, phi_u= (logsigma^2_P logsigma^2_M0 logsigma^2_Meta a_P a_M),  where the variance of zeta_P is sigma^2_P, the variance of the i^th entry of  zeta_M scales with its magnitude:  log sigma^2_Mi = logsigma^2_M0i + logsigma^2_Meta i  mu_zeta_Mi,  and a_P and a_M are parameter vectors of the blocks of the correlation matrix.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In order to account for correlations between global and site-specific parameters, HVI models p(zeta) as a multivariate normal distribution that is a shifted  zero-zentered multivariate normal, p(zeta_r). ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"zeta = (zeta_P zeta_Ms) = zeta_r + (mu_zeta_P mu_zeta_Ms)  \n\nzeta_r = (zeta_rP zeta_rMs)  sim mathcalN(0 diag(sigma_zeta)^T C_zeta  diag(sigma_zeta))\n \nmu_zeta_Ms = g_s(X_M zeta_P phi_g)\n\ndiag(sigma_zeta) = e^(logsigma^2_P logsigma^2_M)2\n\nC_zeta = U^T U\n\nU = operatornameBlockDiag(a_P a_M)","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"where the predicted value of mu_zeta_Ms may depend on the random draw value of zeta_P = zeta_rP + mu_zeta_P. By this construction HVI better supports the assumption that zeta_rM is conditionally independent of zeta_rP, which is required to macke the cholesky-factor, U of the covariance matrix block-diagonal.","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"The above procedure makes an additional subtle approximation. HVI allows the variance of zeta_M to scale with its magnitude. In the computation of the correlation matrix, however, HVI uses the mean, mu_zeta_Mi, rather than the actual sampled value, zeta_Mi. If it used the actual value, then the distribution of zeta would need to be described as a general distribution, p(zeta) = p(zeta_Mszeta_P)  p(zeta_P), that would not  be normal any more, and HVI could not compute the expectation by drawing centered normla random numbers.","category":"page"},{"location":"explanation/theory_hvi/#Implementation-of-the-cost-function","page":"Theory","title":"Implementation of the cost function","text":"","category":"section"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"In practical terms the cost function ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"generates normally distributed random values (zeta_rP zeta_rMs) based on the cholesky factor of the covariance matrix, which depends on optimized parameters (a_P a_M)\ngenerates a sample of zeta_P by adding optimized parameters mu_zeta_P to zeta_rP\ncomputes expected value of mu_zeta_Ms using the machine learning model given covariates, X_M, given zeta_P, and given optimized parameters phi_g.\ngenerates a sample of zeta_Ms by adding the computed mu_zeta_Ms to zeta_rMs\ntransforms (zeta_P zeta_Ms) to the original scale to get a sample of model parameters (theta_rP theta_rMs)\ncomputes negative Log-density of observations for each sample using the physical model, f, and subtract the absolute determinant of the transformation, evaluated at the sample.\napproximates the expected value of the former by taking the mean across the samples\nsubtract the entropy of the normal distribution approximator ","category":"page"},{"location":"explanation/theory_hvi/","page":"Theory","title":"Theory","text":"The automatic differentiation through this cost function including calls to g, T, and f allows to estimate parameters, phi, by a stochastic gradient decent method.","category":"page"},{"location":"tutorials/inspect_results/#Inspect-results-of-fitted-problem","page":"Inspect results","title":"Inspect results of fitted problem","text":"","category":"section"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"This tutorial leads you through querying relevant information from the inversion results and to produce some typical plots.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"First load necessary packages.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"using HybridVariationalInference\nusing StableRNGs\nusing ComponentArrays: ComponentArrays as CA\nusing SimpleChains # for reloading the optimized problem\nusing DistributionFits\nusing JLD2\nusing CairoMakie\nusing PairPlots   # scatterplot matrices","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"This tutorial uses the fitted object saved at the end of the Basic workflow without GPU tutorial.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"fname = \"intermediate/basic_cpu_results.jld2\"\nprint(abspath(fname))\nprobo = load(fname, \"probo\");","category":"page"},{"location":"tutorials/inspect_results/#Sample-the-posterior","page":"Inspect results","title":"Sample the posterior","text":"","category":"section"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"A sample of both, posterior, and predictive posterior can be obtained using function sample_posterior.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"using StableRNGs\nrng = StableRNG(112)\nn_sample_pred = 400\n(; θsP, θsMs) = sample_posterior(rng, probo; n_sample_pred)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Lets look at the results.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"size(θsP), size(θsMs)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"((1, 400), (800, 2, 400))","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"The last dimension is the number of samples, the second-last dimension is the respective parameter. θsMs has an additional dimension denoting the site for which parameters are samples.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"They are ComponentArrays with the parameter dimension names that can be used:","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"θsMs[1,:r1,:] # sample of r1 of the first site","category":"page"},{"location":"tutorials/inspect_results/#Corner-plots","page":"Inspect results","title":"Corner plots","text":"","category":"section"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"The relation between different variables can be well inspected by scatterplot matrices, also called corner plots or pair plots. PairPlots.jl provides a Makie-implementation of those.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Here, we plot the global parameters and the site-parameters for the first site.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"i_site = 1\nθ1 = vcat(θsP, θsMs[i_site,:,:])\nθ1_nt = NamedTuple(k => CA.getdata(θ1[k,:]) for k in keys(θ1[:,1])) # \nplt = pairplot(θ1_nt)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"(Image: )","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"The plot shows that parameters for the first site, K_1 and r_1, are correlated, but that we did not model correlation with the global parameter, K_2.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Note that this plots shows only the first out of 800 sites. HVI estimated a 1602-dimensional posterior distribution including covariances among parameters.","category":"page"},{"location":"tutorials/inspect_results/#Expected-values-and-marginal-variances","page":"Inspect results","title":"Expected values and marginal variances","text":"","category":"section"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Lets look at how the estimated uncertainty of a site parameter changes with its expected value.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"par = :K1\nθmean = [mean(θsMs[s,par,:]) for s in axes(θsMs, 1)]\nθsd = [std(θsMs[s,par,:]) for s in axes(θsMs, 1)]\nfig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean($par)\",ylabel=\"sd($par)\")\nscatter!(ax, θmean, θsd) \nfig","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"(Image: )","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"We see that K_1 across sites ranges from about 0.18 to 0.25, and that its estimated uncertainty is about 0.034, slightly decreasing with the values of the parameter.","category":"page"},{"location":"tutorials/inspect_results/#Predictive-Posterior","page":"Inspect results","title":"Predictive Posterior","text":"","category":"section"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"In addition to the uncertainty in parameters, we are also interested in the uncertainty of predictions, i.e. the predictive posterior.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"We cam either run the PBM for all the parameter samples that we obtained already, using the AbstractModelApplicator, or use predict_hvi which combines sampling the posterior and predictive posterior and returns the additional NamedTuple entry y.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"(; y, θsP, θsMs) = predict_hvi(rng, probo; n_sample_pred)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"size(y)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"(8, 800, 400)","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Again, the last dimension is the sample. The other dimensions correspond to the observations we provided for the fitting: The first dimension is the observation within one site, the second dimension is the site.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"Lets look on how the uncertainty of the 4th observations scales with its predicted magnitude across sites.","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"i_obs = 4\nymean = [mean(y[i_obs,s,:]) for s in axes(θsMs, 1)]\nysd = [std(y[i_obs,s,:]) for s in axes(θsMs, 1)]\nfig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean(y$i_obs)\",ylabel=\"sd(y$i_obs)\")\nscatter!(ax, ymean, ysd) \nfig","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"(Image: )","category":"page"},{"location":"tutorials/inspect_results/","page":"Inspect results","title":"Inspect results","text":"We see that observed values for associated substrate concentrations range about from 0.51 to 0.59 with an estimated standard deviation around 0.005 that decreases with the observed value.","category":"page"},{"location":"tutorials/basic_cpu/#Basic-workflow-without-GPU","page":"Basic workflow","title":"Basic workflow without GPU","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"First load necessary packages.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"using HybridVariationalInference\nusing HybridVariationalInference: HybridVariationalInference as HVI\nusing ComponentArrays: ComponentArrays as CA\nusing Bijectors\nusing StableRNGs\nusing SimpleChains\nusing StatsFuns\nusing MLUtils\nusing DistributionFits","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Next, specify many moving parts of the Hybrid variational inference (HVI)","category":"page"},{"location":"tutorials/basic_cpu/#The-process-based-model","page":"Basic workflow","title":"The process-based model","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The example process based model (PBM) predicts a double-monod constrained rate for different substrate concentrations, S1, and S2.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"\ny = r_0+ r_1 fracS_1K_1 + S_1 fracS_2K_2 + S_2","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"function f_doubleMM(θc::CA.ComponentVector{ET}, x) where ET\n    # extract parameters not depending on order, i.e whether they are in θP or θM\n    (r0, r1, K1, K2) = map((:r0, :r1, :K1, :K2)) do par\n        CA.getdata(θc[par])::ET\n    end\n    r0 .+ r1 .* x.S1 ./ (K1 .+ x.S1) .* x.S2 ./ (K2 .+ x.S2)\nend","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Its formulation is independent of which parameters are global, site-specific, or fixed during the model inversion. However, it cannot assume an ordering in the parameters, but needs to access the components by its symbolic names in the provided ComponentArray.","category":"page"},{"location":"tutorials/basic_cpu/#Likelihood-function","page":"Basic workflow","title":"Likelihood function","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"HVI requires the evaluation of the likelihood of the predictions. It corresponds to the cost of predictions given some observations.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The user specifies a function of the negative log-Likehood neg_logden(obs, pred, uncertainty_parameters), where all of the parameters are arrays with columns for sites.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Here, we use the neg_logden_indep_normal function that assumed observations to be distributed independently normal around a true value. The provided y_unc uncertainty parameters, here, corresponds to logσ2, denoting the log of the variance parameter of the normal distribution.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"py = neg_logden_indep_normal","category":"page"},{"location":"tutorials/basic_cpu/#Global-Site,-transformations,-and-priors","page":"Basic workflow","title":"Global-Site, transformations, and priors","text":"","category":"section"},{"location":"tutorials/basic_cpu/#Global-and-site-specific-parameters","page":"Basic workflow","title":"Global and site-specific parameters","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"In this example, we will assign a fixed value to r0 parameter, treat the K2 parameter as unknown but the same across sites, and predict r1 and K1 for each site separately, based on covariates known at the sites.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Here we provide initial values for them by using ComponentVector.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"FT = Float32\nθM0 = θM = CA.ComponentVector{FT}(r1=0.5, K1=0.2) # separately for each individual\nθP0 = θP = CA.ComponentVector{FT}(K2=2.0)  # population: same across individuals, \nθFix = CA.ComponentVector{FT}(r0=0.3) # r0, i.e. not estimated","category":"page"},{"location":"tutorials/basic_cpu/#Parameter-Transformations","page":"Basic workflow","title":"Parameter Transformations","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"HVI allows for transformations of parameters in an unconstrained space, where the probability density is not strictly zero anywhere to the original constrained space.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Here, our model parameters are strictly positive, and we use the exponential function to transform unconstrained estimates to the original constrained domain.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"transP = Stacked(HVI.Exp())  \ntransM = Stacked(HVI.Exp(), HVI.Exp())","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Parameter transformations are specified using the Bijectors package. Because, Bijectors.elementwise(exp), has problems with automatic differentiation (AD) on GPU, we use the public but non-exported Exp wrapper inside Bijectors.Stacked.","category":"page"},{"location":"tutorials/basic_cpu/#Prior-information-on-parameters-at-constrained-scale","page":"Basic workflow","title":"Prior information on parameters at constrained scale","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"HVI is an approximate bayesian analysis and combines prior information on the parameters with the model and observed data.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Here, we provide a wide prior by fitting a Lognormal distributions to","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"the mean corresponding to the initial value provided above\nthe 0.95-quantile 3 times the mean","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"using the DistributionFits.jl package.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"θall = vcat(θP, θM)\npriors_dict = Dict{Symbol, Distribution}(\n    keys(θall) .=> fit.(LogNormal, θall, QuantilePoint.(θall .* 3, 0.95)))","category":"page"},{"location":"tutorials/basic_cpu/#Observations,-model-drivers-and-covariates","page":"Basic workflow","title":"Observations, model drivers and covariates","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The model parameters are inverted using information on the","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"observed data, y_o\nits uncertainty, y_unc\nknown covariates across sites, xM\nmodel drivers, xP","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Here, we use synthetic data generated by the package.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"rng = StableRNG(111)\n(; xM, xP, y_o, y_unc) = gen_hybridproblem_synthetic(\n    rng, DoubleMM.DoubleMMCase(); scenario=Val((:omit_r0,)))","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Lets look at them.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"size(xM), size(xP), size(y_o), size(y_unc)","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"((5, 800), (16, 800), (8, 800), (8, 800))","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"All of them have 800 columns, corresponding to 800 sites. There are 5 site-covaritas, 16 values of model drivers, and 8 observations per site.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"xP[:,1]","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"ComponentVector{Float32}(S1 = Float32[0.5, 0.5, 0.5, 0.5, 0.4, 0.3, 0.2, 0.1], S2 = Float32[1.0, 3.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0])","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"In each column of the model drivers there is a ComponentVector with components S1 and S2 corresponding to the concentrations, for which outputs were observed. This allows notation x.S1 in the PBM above.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The y_unc becomes its meaning by the Likelihood-function to be specified with the problem below.","category":"page"},{"location":"tutorials/basic_cpu/#Providing-data-in-batches","page":"Basic workflow","title":"Providing data in batches","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"HVI uses MLUtils.DataLoader to provide baches of the data during each iteration of the solver. In addition to the data, it provides an index to the sites inside a tuple.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"n_site = size(y_o,2)\nn_batch = 20\ntrain_dataloader = MLUtils.DataLoader(\n    (xM, xP, y_o, y_unc, 1:n_site), batchsize=n_batch, partial=false)","category":"page"},{"location":"tutorials/basic_cpu/#The-Machine-Learning-model","page":"Basic workflow","title":"The Machine-Learning model","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The machine-learning (ML) part predicts parameters of the posterior of site-specific PBM parameters, given the covariates. Here, we specify a 3-layer feed-forward neural network using the SimpleChains framework which works efficiently on CPU.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"n_out = length(θM) # number of individuals to predict \nn_input = n_covar = size(xM,1)\n\ng_chain = SimpleChain(\n    static(n_input), # input dimension (optional)\n    TurboDense{true}(tanh, n_input * 4),\n    TurboDense{true}(tanh, n_input * 4),\n    # dense layer without bias that maps to n outputs to (0..1)\n    TurboDense{false}(logistic, n_out)\n)\n# get a template of the parameter vector, ϕg0\ng_chain_app, ϕg0 = construct_ChainsApplicator(rng, g_chain)","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The g_chain_app ChainsApplicator predicts the parameters of the posterior, approximation given a vector of ML weights,ϕg. During construction, an initial template of this vector is created. This abstraction layer allows to use different ML frameworks and replace the SimpleChains model by Flux or Lux.","category":"page"},{"location":"tutorials/basic_cpu/#Using-priors-to-scale-ML-parameter-estimates","page":"Basic workflow","title":"Using priors to scale ML-parameter estimates","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"In order to balance gradients, the g_chain_app ModelApplicator defined above predicts on a scale (0..1). Now the priors are used to translate this to the parameter range by using the cumulative density distribution.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Priors were specified at constrained scale, but the ML model predicts parameters on unconstrained scale. This transformation of the distribution can be mathematically worked out for specific prior distribution forms. However, for simplicity, a NormalScalingModelApplicator is fitted to the transformed 5% and 95% quantiles of the original prior.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"priorsM = [priors_dict[k] for k in keys(θM)]\nlowers, uppers = get_quantile_transformed(priorsM, transM)\ng_chain_scaled = NormalScalingModelApplicator(g_chain_app, lowers, uppers, FT)","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The g_chain_scaled ModelApplicator now predicts in unconstrained scale, transforms logistic predctions around 0.5 to the range of high prior probability of the parameters, and transforms ML predictions near 0 or 1 towards the outer lower probability ranges.","category":"page"},{"location":"tutorials/basic_cpu/#Assembling-the-information","page":"Basic workflow","title":"Assembling the information","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"All the specifications above are stored in a HybridProblem structure.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Before, a PBMSiteApplicator is constructed that translates an invocation given a vector of global parameters, and a matrix of site parameters to invocation of the process based model (PBM), defined at the beginning.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"f_batch = f_allsites = PBMSiteApplicator(f_doubleMM; θP, θM, θFix, xPvec=xP[:,1])\n\nprob = HybridProblem(θP, θM, g_chain_scaled, ϕg0, \n    f_batch, f_allsites, priors_dict, py,\n    transM, transP, train_dataloader, n_covar, n_site, n_batch)","category":"page"},{"location":"tutorials/basic_cpu/#Perform-the-inversion","page":"Basic workflow","title":"Perform the inversion","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Eventually, having assembled all the moving parts of the HVI, we can perform the inversion.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"using OptimizationOptimisers\nimport Zygote\n\nsolver = HybridPosteriorSolver(; alg=Adam(0.02), n_MC=3)\n\n(; probo, interpreters) = solve(prob, solver; rng,\n    callback = callback_loss(100), # output during fitting\n    epochs = 2,\n);","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"The solver object is constructed given the specific stochastic optimization algorithm and the number of Monte-Carlo samples that are drawn in each iteration from the predicted parameter posterior.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Then the solver is applied to the problem using solve for a given number of iterations or epochs. For this tutorial, we additionally specify that the function to transfer structures to the GPU is the identity function, so that all stays on the CPU, and this tutorial hence does not require ad GPU or GPU livraries.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Among the return values are","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"probo: A copy of the HybridProblem, with updated optimized parameters\ninterpreters: A NamedTuple with several ComponentArrayInterpreters that","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"will help analyzing the results.","category":"page"},{"location":"tutorials/basic_cpu/#Using-a-population-level-process-based-model","page":"Basic workflow","title":"Using a population-level process-based model","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"So far, the process-based model ram for each single site. For this simple model, some performance grains result from matrix-computations when running the model for all sites within one batch simultaneously.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"In the following, the PBM specification accepts matrices as arguments for parameters and drivers and returns a matrix of precitions. For the parameters, one row corresponds to one site. For the drivers and predictions, one column corresponds to one site.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"function f_doubleMM_sites(θc::CA.ComponentMatrix, xPc::CA.ComponentMatrix)\n    # extract several covariates from xP\n    ST = typeof(CA.getdata(xPc)[1:1,:])  # workaround for non-type-stable Symbol-indexing\n    S1 = (CA.getdata(xPc[:S1,:])::ST)   \n    S2 = (CA.getdata(xPc[:S2,:])::ST)\n    #\n    # extract the parameters as row-repeated vectors\n    n_obs = size(S1, 1)\n    VT = typeof(CA.getdata(θc)[:,1])   # workaround for non-type-stable Symbol-indexing\n    (r0, r1, K1, K2) = map((:r0, :r1, :K1, :K2)) do par\n        p1 = CA.getdata(θc[:, par]) ::VT\n        repeat(p1', n_obs)  # matrix: same for each concentration row in S1\n    end\n    #\n    # each variable is a matrix (n_obs x n_site)\n    r0 .+ r1 .* S1 ./ (K1 .+ S1) .* S2 ./ (K2 .+ S2)\nend","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Again, the function should not rely on the order of parameters but use symbolic indexing to extract the parameter vectors. For type stability of this symbolic indexing, it uses a workaround to get the type of a single row. Similarly, it uses type hints to index into the drivers, xPc, to extract sub-matrices by symbols. Alternatively, here it could rely on the structure and ordering of the columns in xPc.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"A corresponding PBMPopulationApplicator transforms calls with partitioned global and site parameters to calls of this matrix version of the PBM. The HVI Problem needs to be updated with this new applicatior.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"f_batch = PBMPopulationApplicator(f_doubleMM_sites, n_batch; θP, θM, θFix, xPvec=xP[:,1])\nf_allsites = PBMPopulationApplicator(f_doubleMM_sites, n_site; θP, θM, θFix, xPvec=xP[:,1])\nprobo_sites = HybridProblem(probo; f_batch, f_allsites)","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"For numerical efficiency, the number of sites within one batch is part of the PBMPopulationApplicator. Hence, we have two different functions, one applied to a batch of site, and another applied to all sites.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"As a test of the new applicator, the results are refined by running a few more epochs of the optimization.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"(; probo) = solve(probo_sites, solver; rng,\n    callback = callback_loss(100), # output during fitting\n    epochs = 20,\n    #is_inferred = Val(true), # activate type-checks \n);","category":"page"},{"location":"tutorials/basic_cpu/#Saving-the-results","page":"Basic workflow","title":"Saving the results","text":"","category":"section"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Extracting useful information from the optimized HybridProblem is covered in the following Inspect results of fitted problem tutorial. In order to use the results from this tutorial in other tutorials, the updated probo HybridProblem and the interpreters are saved to a JLD2 file.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"Before the problem is updated to use the redefinition DoubleMM.f_doubleMM_sites of the PBM in module DoubleMM rather than module Main to allow for easier reloading with JLD2.","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"f_batch = PBMPopulationApplicator(DoubleMM.f_doubleMM_sites, n_batch; θP, θM, θFix, xPvec=xP[:,1])\nf_allsites = PBMPopulationApplicator(DoubleMM.f_doubleMM_sites, n_site; θP, θM, θFix, xPvec=xP[:,1])\nprobo2 = HybridProblem(probo; f_batch, f_allsites)","category":"page"},{"location":"tutorials/basic_cpu/","page":"Basic workflow","title":"Basic workflow","text":"using JLD2\nfname = \"intermediate/basic_cpu_results.jld2\"\nmkpath(\"intermediate\")\nif probo2 isa AbstractHybridProblem # do not save on failure above\n    jldsave(fname, false, IOStream; probo=probo2, interpreters)\nend","category":"page"},{"location":"tutorials/lux_gpu/#How-to-move-computations-to-GPU","page":".. use GPU","title":"How to move computations to GPU","text":"","category":"section"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"This guide shows how to configure the setup and inversion of a HybridProblem so that computations of the ML model and maybe also the process-based model are executed on GPU.","category":"page"},{"location":"tutorials/lux_gpu/#Motivation","page":".. use GPU","title":"Motivation","text":"","category":"section"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"Machine learning is often accelerated by moving computations form CPU to GPU. So does HVI.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"First load necessary packages.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"using HybridVariationalInference\nusing ComponentArrays: ComponentArrays as CA\nusing Bijectors\nusing Lux\nusing SimpleChains # only loading save object\nusing StatsFuns\nusing StableRNGs\nusing MLUtils\nusing JLD2\nusing Random\n# using CairoMakie\n# using PairPlots   # scatterplot matrices","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"This tutorial reuses and modifies the fitted object saved at the end of the Basic workflow without GPU tutorial.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"fname = \"intermediate/basic_cpu_results.jld2\"\nprint(abspath(fname))\nprob = probo_chain = load(fname, \"probo\");","category":"page"},{"location":"tutorials/lux_gpu/#Updating-the-ML-model-of-the-problem-to-use-LUX","page":".. use GPU","title":"Updating the ML model of the problem to use LUX","text":"","category":"section"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"Because the SimpleChains ML model used in the basic tutorial does not support GPU, we reconstruct the model using the LUX framework. Note that all the setup is almost the same, as in the basic workflow. The only difference is that a Lux.Chains object is provided to construct_ChainsApplicator.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"n_out = length(prob.θM) # number of individuals to predict \nn_covar = 5 #size(xM,1)\nn_input = n_covar \n\ng_lux = Lux.Chain(\n    Lux.Dense(n_covar => n_covar * 4, tanh),\n    Lux.Dense(n_covar * 4 => n_covar * 4, tanh),\n    Lux.Dense(n_covar * 4 => n_out, logistic, use_bias = false)\n)\n# get a template of the parameter vector, ϕg0\nrng = StableRNG(111)\ng_chain_app, ϕg0 = construct_ChainsApplicator(rng, g_lux)\n#\npriorsM = [prob.priors[k] for k in keys(prob.θM)]\nlowers, uppers = get_quantile_transformed(priorsM, prob.transM)\nFT = eltype(prob.θM)\ng_chain_scaled = NormalScalingModelApplicator(g_chain_app, lowers, uppers, FT)","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"Update the HybridProblem to use this ML model.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"prob_lux = HybridProblem(probo_chain; g=g_chain_scaled, ϕg=ϕg0)","category":"page"},{"location":"tutorials/lux_gpu/#Specifying-GPU-devices-during-solve","page":".. use GPU","title":"Specifying GPU devices during solve","text":"","category":"section"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"The solve method for the HybridPosteriorSolver accepts argument gdevs, Its a NamedTuple with entriesgdev_M and gdev_P, for the ML model on and the process-basee model (PBM) respectively. They specify functions that are applied to move callables and data to GPU.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"They default to identity, meaning that nothing is moved from CPU to GPU. Function gpu_device() from package MLDataDevices can be used instead for the standard GPU device.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"Hence specify","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"gdevs = (; gdev_M=gpu_device(), gdev_P=gpu_device()): to move both ML model and PBM to GPU\ngdevs = (; gdev_M=gpu_device(), gdev_P=identity): to move both ML model to GPU but execute the PBM (and parameter transformation) on CPU","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"In addition, the libraries of the GPU device need to be activated by importing respective Julia packages. Currently, only CUDA is tested with this HybridVariationalInference package.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"import CUDA, cuDNN # so that gpu_device() returns a CUDADevice\n#CUDA.device!(4)\ngdevs = (; gdev_M=gpu_device(), gdev_P=gpu_device())\n#gdevs = (; gdev_M=gpu_device(), gdev_P=identity)\n\nusing OptimizationOptimisers\nimport Zygote\nsolver = HybridPosteriorSolver(; alg=Adam(0.02), n_MC=3)\n\n(; probo) = solve(prob_lux, solver; \n    callback = callback_loss(100), \n    epochs = 10,\n    gdevs,\n); probo_lux = probo;","category":"page"},{"location":"tutorials/lux_gpu/#Moving-results-from-GPU-to-CPU","page":".. use GPU","title":"Moving results from GPU to CPU","text":"","category":"section"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"The sampling and prediction methods, also take this gdevs keyword argument.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"n_sample_pred = 400\n(y_dev, θsP_dev, θsMs_dev) = (; y, θsP, θsMs) = predict_hvi(\n  rng, probo_lux; n_sample_pred, gdevs);","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"If gdev_P is not an AbstractGPUDevice then all the results are on CPU. If gdev_P is an AbstractGPUDevice then the results are GPUArrays and need to be transferred to CPU.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"typeof(θsMs_dev)","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"ComponentArrays.ComponentArray{Float32, 3, CUDA.CuArray{Float32, 3, CUDA.DeviceMemory}, Tuple{ComponentArrays.Axis{(i = 1:800,)}, ComponentArrays.Axis{(r1 = 1, K1 = 2)}, ComponentArrays.Axis{(i = 1:400,)}}}","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"Handling of a ComponentArrays backed by GPUArrays can result in errors of scalar indexing. Therefore, use a semicolon to suppress printing. Also for moving the ComponentArrays to CPU, use function apply_preserve_axes to circumvent this error.","category":"page"},{"location":"tutorials/lux_gpu/","page":".. use GPU","title":".. use GPU","text":"cdev = cpu_device()\ny = cdev(y_dev)\nθsP = apply_preserve_axes(cdev, θsP_dev)\nθsMs = apply_preserve_axes(cdev, θsMs_dev)","category":"page"},{"location":"tutorials/blocks_corr/#How-to-model-indenpendent-parameter-blocks-in-the-posterior","page":".. model independent parameters","title":"How to model indenpendent parameter-blocks in the posterior","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"This guide shows how to configure independent parameter-blocks in the correlations of the posterior.","category":"page"},{"location":"tutorials/blocks_corr/#Motivation","page":".. model independent parameters","title":"Motivation","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"Modelling all correlations among global and site PBM-parameters respectively requires many degrees of freedom.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"To decrease the number of parameters to estimate, HVI allows to decompose the correlations into independent sub-blocks of parameters.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"First load necessary packages.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"using HybridVariationalInference\nusing ComponentArrays: ComponentArrays as CA\nusing Bijectors\nusing SimpleChains\nusing MLUtils\nusing JLD2\nusing Random\nusing CairoMakie\nusing PairPlots   # scatterplot matrices","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"This tutorial reuses and modifies the fitted object saved at the end of the Basic workflow without GPU tutorial.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"fname = \"intermediate/basic_cpu_results.jld2\"\nprint(abspath(fname))\nprob = probo_cor = load(fname, \"probo\");","category":"page"},{"location":"tutorials/blocks_corr/#Specifying-blocks-in-correlation-structure","page":".. model independent parameters","title":"Specifying blocks in correlation structure","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"HVI models the posterior of the parameters at unconstrained scale using a multivariate normal distribution. It estimates a parameterization of the associated blocks in the correlation matrx and requires a specification of the block-structure.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"This is done by specifying the positions of the end of the blocks for the global (P) and the site-specific parameters (M) respectively using a NamedTuple of integer vectors.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"The defaults specifies a single entry, meaning, there is only one big block respectively, spanning all parameters.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"cor_ends0 = (P=[length(prob.θP)], M=[length(prob.θM)])","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(P = [1], M = [2])","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"The following specification models one-entry blocks for each each parameter in the correlation block the site parameters, i.e. treating all parameters independently with not modelling any correlations between them.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"cor_ends = (P=[length(prob.θP)], M=1:length(prob.θM))","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(P = [1], M = 1:2)","category":"page"},{"location":"tutorials/blocks_corr/#Reinitialize-parameters-for-the-posterior-approximation.","page":".. model independent parameters","title":"Reinitialize parameters for the posterior approximation.","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"HVI uses additional fitted parameters to represent the means and the covariance matrix of the posterior distribution of model parameters. With fewer correlations, also the number of those parameters changes, and those parameters must be reinitialized after changing the block structure in the correlation matrix.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"Here, we obtain construct initial estimates. using init_hybrid_ϕunc","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"ϕunc = init_hybrid_ϕunc(cor_ends, zero(eltype(prob.θM)))","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"In this two-site parameter case, the the blocked structure saves only one degree of freedom:","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"length(ϕunc), length(probo_cor.ϕunc)","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(5, 6)","category":"page"},{"location":"tutorials/blocks_corr/#Update-the-problem-and-redo-the-inversion","page":".. model independent parameters","title":"Update the problem and redo the inversion","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"prob_ind = HybridProblem(prob; cor_ends, ϕunc)","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"using OptimizationOptimisers\nimport Zygote\n\nsolver = HybridPosteriorSolver(; alg=Adam(0.02), n_MC=3)\n\n(; probo) = solve(prob_ind, solver; \n    callback = callback_loss(100), # output during fitting\n    epochs = 20,\n); probo_ind = probo;","category":"page"},{"location":"tutorials/blocks_corr/#Compare-the-correated-vs.-uncorrelated-posterior","page":".. model independent parameters","title":"Compare the correated vs. uncorrelated posterior","text":"","category":"section"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"First, draw a sample.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"n_sample_pred = 400\n(y_cor, θsP_cor, θsMs_cor) = (; y, θsP, θsMs) = predict_hvi(\n  Random.default_rng(), probo_cor; n_sample_pred)\n(y_ind, θsP_ind, θsMs_ind) = (; y, θsP, θsMs) = predict_hvi(\n  Random.default_rng(), probo_ind; n_sample_pred)","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"i_site = 1\nθ1 = vcat(θsP_ind, θsMs_ind[i_site,:,:])\nθ1_nt = NamedTuple(k => CA.getdata(θ1[k,:]) for k in keys(θ1[:,1])) # \nplt = pairplot(θ1_nt)","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(Image: )","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"The corner plot of the independent-parameters estimate shows no correlations between site parameters, r₁ and K₁.","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"i_out = 4\nfig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean(y)\",ylabel=\"sd(y)\")\nymean_cor = [mean(y_cor[i_out,s,:]) for s in axes(y_cor, 2)]\nysd_cor = [std(y_cor[i_out,s,:]) for s in axes(y_cor, 2)]\nscatter!(ax, ymean_cor, ysd_cor, label=\"correlated\") \nymean_ind = [mean(y_ind[i_out,s,:]) for s in axes(y_ind, 2)]\nysd_ind = [std(y_ind[i_out,s,:]) for s in axes(y_ind, 2)]\nscatter!(ax, ymean_ind, ysd_ind, label=\"independent\") \naxislegend(ax, unique=true)\nfig","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(Image: )","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"plot_sd_vs_mean = (par) -> begin\n  fig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean($par)\",ylabel=\"sd($par)\")\n  θmean_cor = [mean(θsMs_cor[s,par,:]) for s in axes(θsMs_cor, 1)]\n  θsd_cor = [std(θsMs_cor[s,par,:]) for s in axes(θsMs_cor, 1)]\n  scatter!(ax, θmean_cor, θsd_cor, label=\"correlated\") \n  θmean_ind = [mean(θsMs_ind[s,par,:]) for s in axes(θsMs_ind, 1)]\n  θsd_ind = [std(θsMs_ind[s,par,:]) for s in axes(θsMs_ind, 1)]\n  scatter!(ax, θmean_ind, θsd_ind, label=\"independent\") \n  axislegend(ax, unique=true)\n  fig\nend\nplot_sd_vs_mean(:K1)","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"(Image: )","category":"page"},{"location":"tutorials/blocks_corr/","page":".. model independent parameters","title":".. model independent parameters","text":"The inversion that neglects correlations among site parameters results in the same magnitude of estimated uncertainty of predictions. However, the uncertainty of the model parameters is severely underestimated in this example.","category":"page"},{"location":"problem/#Problem","page":"Problem","title":"Problem","text":"","category":"section"},{"location":"problem/","page":"Problem","title":"Problem","text":"Consider the case of Parameter learning, a special case of hybrid models,  where a machine learning model, g_phi_g, uses known covariates x_Mi at site i,   to predict a subset of the parameters, theta of the process based model, f.","category":"page"},{"location":"problem/","page":"Problem","title":"Problem","text":"We are interested in both,","category":"page"},{"location":"problem/","page":"Problem","title":"Problem","text":"the uncertainty of hybrid model predictions, ŷ (predictive posterior), and\nthe uncertainty of process-model parameters theta, including their correlations (posterior)","category":"page"},{"location":"problem/","page":"Problem","title":"Problem","text":"For example we have soil organic matter process-model that predicts carbon stocks for  different sites. We need to parameterize the unknown carbon use efficiency (CUE) of the soil microbial community that differs by site, but is hypothesized to correlate with climate variables and pedogenic factors, such as clay content. We apply a machine learning model to estimate CUE and fit it end-to-end with other parameters of the process-model to observed carbon stocks. In addition to the predicted CUE, we are interested in the uncertainty of CUE and its correlation with other parameters, such a the capacity of the soil minerals to bind carbon.  I.e. we are interetes in the entire posterior probability distribution of the model parameters.","category":"page"},{"location":"tutorials/corr_site_global/#How-to-account-for-correlations-between-site-and-global-parameters","page":".. model site-global corr","title":"How to account for correlations between site and global parameters","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"This guide shows how to configure providing certain global parameters as covariates to the ML model.","category":"page"},{"location":"tutorials/corr_site_global/#Motivation","page":".. model site-global corr","title":"Motivation","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"HVI requires the site-parameter blocks in the correlation matrix to be independent of the global-parameter block.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"However, correlations between site and global parameters can be modeled by predicting different (transformed) site parameters, zeta_Ms , given the sampled (transformed) global parameters, zeta_P.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"\np(zeta_Ms zeta_P) = p(zeta_Ms  zeta_P) p(zeta_P)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"This comes at the cost of running the ML model forward for each sampled global parameter, rather than just once in each sampling of the posterior.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"First load necessary packages.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"using HybridVariationalInference\nusing ComponentArrays: ComponentArrays as CA\nusing Bijectors\nusing SimpleChains\nusing StatsFuns\nusing StableRNGs\nusing MLUtils\nusing JLD2\nusing Random\nusing CairoMakie\nusing PairPlots   # scatterplot matrices","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"This tutorial reuses and modifies the fitted object saved at the end of the Basic workflow without GPU tutorial.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"fname = \"intermediate/basic_cpu_results.jld2\"\nprint(abspath(fname))\nprob = probo_uncond = load(fname, \"probo\");","category":"page"},{"location":"tutorials/corr_site_global/#The-pbm_covars-entry-in-the-HybridProblem","page":".. model site-global corr","title":"The pbm_covars entry in the HybridProblem","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"When constructing a HybridProblem, the names of the subset of global parameters provided to the ML model is specified using the pbm_covars argument. It defaults to the empty tuple (), meaning that no global parameters are provided as inputs.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"The following specification tells, that the K_2 parameter should be provided as input to the ML model.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"pbm_covars = (:K2,)","category":"page"},{"location":"tutorials/corr_site_global/#The-modified-Machine-Learning-model","page":".. model site-global corr","title":"The modified Machine-Learning model","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"The ML model needs to be adapted to consume more inputs than the site covariates. Note the changed n_input specification compared to the Basic workflow without GPU tutorial.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"n_out = length(prob.θM) # number of individuals to predict \nn_covar = 5 #size(xM,1)\nn_input = n_covar + length(pbm_covars)  \nrng = StableRNG(111)\n\ng_chain = SimpleChain(\n    static(n_input), # input dimension (optional)\n    TurboDense{true}(tanh, n_input * 4),\n    TurboDense{true}(tanh, n_input * 4),\n    # dense layer without bias that maps to n outputs to (0..1)\n    TurboDense{false}(logistic, n_out)\n)\n# get a template of the parameter vector, ϕg0\ng_chain_app, ϕg0 = construct_ChainsApplicator(rng, g_chain)\n#\npriorsM = [prob.priors[k] for k in keys(prob.θM)]\nlowers, uppers = get_quantile_transformed(priorsM, prob.transM)\nFT = eltype(prob.θM)\ng_chain_scaled = NormalScalingModelApplicator(g_chain_app, lowers, uppers, FT)","category":"page"},{"location":"tutorials/corr_site_global/#Update-the-problem-and-redo-the-inversion","page":".. model site-global corr","title":"Update the problem and redo the inversion","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"prob_cond = HybridProblem(probo_uncond; g=g_chain_scaled, ϕg=ϕg0, pbm_covars)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"using OptimizationOptimisers\nimport Zygote\n\nsolver = HybridPosteriorSolver(; alg=Adam(0.02), n_MC=3)\n\n(; probo) = solve(probo_uncond, solver; \n    callback = callback_loss(100), # output during fitting\n    epochs = 20,\n); probo_cond = probo;","category":"page"},{"location":"tutorials/corr_site_global/#Compare-the-conditional-vs-unconditional-posterior","page":".. model site-global corr","title":"Compare the conditional vs unconditional posterior","text":"","category":"section"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"First, draw a sample.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"n_sample_pred = 400\n(y_cond, θsP_cond, θsMs_cond) = (; y, θsP, θsMs) = predict_hvi(\n  Random.default_rng(), probo_cond; n_sample_pred)\n(y_uncond, θsP_uncond, θsMs_uncond) = (; y, θsP, θsMs) = predict_hvi(\n  Random.default_rng(), probo_uncond; n_sample_pred)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"i_site = 1\nθ1 = vcat(θsP_uncond, θsMs_uncond[i_site,:,:])\nθ1_nt = NamedTuple(k => CA.getdata(θ1[k,:]) for k in keys(θ1[:,1])) # \nplt = pairplot(θ1_nt)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"(Image: )","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"The corner plot of the independent-parameters estimate shows that despite providing K_2 to the ML model, it has only very weak correlations with the site parameters, r_1 and K_1.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"i_out = 4\nfig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean(y)\",ylabel=\"sd(y)\")\nymean_cond = [mean(y_cond[i_out,s,:]) for s in axes(y_cond, 2)]\nysd_cond = [std(y_cond[i_out,s,:]) for s in axes(y_cond, 2)]\nscatter!(ax, ymean_cond, ysd_cond, label=\"conditional\") \nymean_uncond = [mean(y_uncond[i_out,s,:]) for s in axes(y_uncond, 2)]\nysd_uncond = [std(y_uncond[i_out,s,:]) for s in axes(y_uncond, 2)]\nscatter!(ax, ymean_uncond, ysd_uncond, label=\"unconditional\") \naxislegend(ax, unique=true)\nfig","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"(Image: )","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"plot_sd_vs_mean = (par) -> begin\n  fig = Figure(); ax = Axis(fig[1,1], xlabel=\"mean($par)\",ylabel=\"sd($par)\")\n  θmean_cond = [mean(θsMs_cond[s,par,:]) for s in axes(θsMs_cond, 1)]\n  θsd_cond = [std(θsMs_cond[s,par,:]) for s in axes(θsMs_cond, 1)]\n  scatter!(ax, θmean_cond, θsd_cond, label=\"conditional\") \n  θmean_uncond = [mean(θsMs_uncond[s,par,:]) for s in axes(θsMs_uncond, 1)]\n  θsd_uncond = [std(θsMs_uncond[s,par,:]) for s in axes(θsMs_uncond, 1)]\n  scatter!(ax, θmean_uncond, θsd_uncond, label=\"unconditional\") \n  axislegend(ax, unique=true)\n  fig\nend\nplot_sd_vs_mean(:r1)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"(Image: )","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"plot_sd_vs_mean(:K1)","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"(Image: )","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"The “conditional” scenario allows for correlations between global and site parameters by making precitions of site parameters conditional on sampled global parameters.","category":"page"},{"location":"tutorials/corr_site_global/","page":".. model site-global corr","title":".. model site-global corr","text":"It estimates slightly higher marginal uncertainties, both of the predictions and of the model parameters.","category":"page"},{"location":"#HybridVariationalInference","page":"Home","title":"HybridVariationalInference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for HybridVariationalInference.","category":"page"},{"location":"reference/reference_internal/#Reference-of-internal-functions","page":"Internal","title":"Reference of internal functions","text":"","category":"section"},{"location":"reference/reference_internal/","page":"Internal","title":"Internal","text":"In this reference, you will find a detailed overview of internal functions. They are documented here mostly for development of the package. They are not part of the public API and may change without notice. ","category":"page"},{"location":"reference/reference_internal/#CommonSolve.solve-Union{Tuple{is_infer}, Tuple{scen}, Tuple{AbstractHybridProblem, HybridPosteriorSolver}} where {scen, is_infer}","page":"Internal","title":"CommonSolve.solve","text":"solve(prob::AbstractHybridProblem, solver::HybridPosteriorSolver; ...)\n\nPerform the inversion of HVI Problem.\n\nOptional keyword arguments\n\nscenario: Scenario to query prob, defaults to Val(()).\nrng: Random generator, defaults to Random.default_rng().\ngdevs: NamedTuple (;gdev_M, gdev_P) functions to move computation and data of ML model on and PBM respectively to gpu (e.g. gpu_device() or cpu (identity).  defaults to get_gdev_MP(scenario)\nθmean_quant default to 0.0: deprecated\nis_inferred: set to Val(true) to activate type stability checks\n\nReturns a NamedTuple of\n\nprobo: A copy of the HybridProblem, with updated optimized parameters\ninterpreters:  TODO\nϕ: the optimized HVI parameters: a ComponentVector with entries\nμP: ComponentVector of the mean global PBM parameters at unconstrained scale\nϕg: The MLmodel parameter vector, \nunc: ComponentVector of further uncertainty parameters\nθP: ComponentVector of the mean global PBM parameters at constrained scale\nresopt: the structure returned by Optimization.solve. It can contain more information on convergence.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.as_ca","page":"Internal","title":"HybridVariationalInference.as_ca","text":"as_ca(v::AbstractArray, interpretor)\n\nReturns a ComponentArray with underlying data v.\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_internal/#HybridVariationalInference.compose_axes-Tuple{NamedTuple}","page":"Internal","title":"HybridVariationalInference.compose_axes","text":"compose_axes(axtuples::NamedTuple)\n\nCreate a new 1d-axis that combines several other named axes-tuples such as of key = getaxes(::AbstractComponentArray).\n\nThe new axis consists of several ViewAxes. If an axis-tuple consists only of one axis, it is used for the view. Otherwise a ShapedAxis is created with the axes-length of the others, essentially dropping component information that might be present in the dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.compute_cholcor_coefficient_single-Tuple{Any}","page":"Internal","title":"HybridVariationalInference.compute_cholcor_coefficient_single","text":"Compute the cholesky-factor parameter for a given single correlation in a 2x2 matrix. Invert the transformation of cholesky-factor parameterization.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.compute_elbo_components-Tuple{AbstractHybridProblem, HybridPosteriorSolver}","page":"Internal","title":"HybridVariationalInference.compute_elbo_components","text":"Compute the components of the elbo for given initial conditions of the problems for the first batch of the trainloader.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.construct_priors_θ_mean-Union{Tuple{scen}, NTuple{8, Any}} where scen","page":"Internal","title":"HybridVariationalInference.construct_priors_θ_mean","text":"In order to let mean of θ stay close to initial point parameter estimates  construct a prior on mean θ to a Normal around initial prediction.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.front","page":"Internal","title":"HybridVariationalInference.front","text":"omit the last n elements of an iterator\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_internal/#HybridVariationalInference.generate_ζ-Union{Tuple{MT}, Tuple{FT}, Tuple{Any, Any, AbstractVector{FT}, MT}} where {FT, MT}","page":"Internal","title":"HybridVariationalInference.generate_ζ","text":"Generate samples of (inv-transformed) model parameters, ζ,  and the vector of standard deviations, σ, i.e. the diagonal of the cholesky-factor.\n\nAdds the MV-normally distributed residuals, retrieved by sample_ζresid_norm to the means extracted from parameters and predicted by the machine learning model. \n\nThe output shape of size (n_site x n_par x n_MC) is tailored to iterating each MC sample and then transforming each parameter on block across sites.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.get_loss_elbo-NTuple{6, Any}","page":"Internal","title":"HybridVariationalInference.get_loss_elbo","text":"Create a loss function for parameter vector ϕ, given \n\ng(x, ϕ): machine learning model \ntransPMS: transformation from unconstrained space to parameter space\nf(θMs, θP): mechanistic model \ninterpreters: assigning structure to pure vectors, see neg_elbo_gtf\nn_MC: number of Monte-Carlo sample to approximate the expected value across distribution\npbm_covars: tuple of symbols of process-based parameters provided to the ML model\nθP: ComponentVector as a template to select indices of pbm_covars\n\nThe loss function takes in addition to ϕ, data that changes with minibatch\n\nrng: random generator\nxM: matrix of covariates, sites in columns\nxP: drivers for the processmodel: Iterator of size n_site\ny_o, y_unc: matrix of observations and uncertainties, sites in columns\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.gtrans-NTuple{4, Any}","page":"Internal","title":"HybridVariationalInference.gtrans","text":"composition transM ∘ g: transformation after machine learning parameter prediction Provide a transMs = StackedArray(transM, n_batch)\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.invsumn-Tuple{T} where T","page":"Internal","title":"HybridVariationalInference.invsumn","text":"Inverse of s = sumn(n) for positive integer n.\n\nGives an inexact error, if given s was not such a sum.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.neg_elbo_ζtf-NTuple{8, Any}","page":"Internal","title":"HybridVariationalInference.neg_elbo_ζtf","text":"Compute the neg_elbo for each sampled parameter vector (last dimension of ζs).\n\nTransform and compute log-jac\ncall forward model\ncompute log-density of predictions\ncompute entropy of transformation\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.sample_ζresid_norm-Tuple{Random.AbstractRNG, AbstractVector, AbstractMatrix, Vararg{Any}}","page":"Internal","title":"HybridVariationalInference.sample_ζresid_norm","text":"Extract relevant parameters from ζ and return nMC generated multivariate normal draws together with the vector of standard deviations, σ: `(ζPresids, ζMsparfirstresids, σ)The output shape(nθ, nsite?, nMC)is tailored to addingζMsparfirstresidsto ML-model predcitions of size(nθM, n_site)`.\n\nArguments\n\nint_unc: Interpret vector as ComponentVector with components  ρsP, ρsM, logσ2ζP, coeflogσ2_ζMs(intercept + slope), \n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.transformU_block_cholesky1-Union{Tuple{AbstractVector{T}}, Tuple{TI}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{TI}}} where {T, TI<:Integer}","page":"Internal","title":"HybridVariationalInference.transformU_block_cholesky1","text":"transformU_block_cholesky1(v::AbstractVector, cor_ends)\n\nTransform a parameterization v of a blockdiagonal of upper triangular matrices into the this matrix. cor_ends is an AbstractVector of Integers specifying the last column of each block.  E.g. For a matrix with a 3x3, a 2x2, and another single-entry block,  the blocks start at columns (3,5,6). It defaults to a single entire block.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.transformU_cholesky1-Tuple{AbstractVector}","page":"Internal","title":"HybridVariationalInference.transformU_cholesky1","text":"Takes a vector of parameters for UnitUpperTriangular matrix and transforms it to an UpperTriangular that satisfies  diag(U' * U) = 1.\n\nThis can be used to fit parameters that yield an upper Cholesky-Factor of a Covariance matrix.\n\nIt uses the upper triangular matrix rather than the lower because it involes a sum across columns, whereas the alternative of a lower triangular uses sum across rows.  Sum across columns is often faster, because entries of columns are contiguous.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.transform_ζs-Tuple{AbstractMatrix, AbstractArray}","page":"Internal","title":"HybridVariationalInference.transform_ζs","text":"Transform parameters \n\nfrom unconstrained (e.g. log) ζ scale of format ((nsite x npar) x n_mc)\nto constrained θ scale of the same format\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.transpose_mPMs_sitefirst-Tuple{Any, Integer, Any, Any, Any}","page":"Internal","title":"HybridVariationalInference.transpose_mPMs_sitefirst","text":"Transforms each row of a matrix (nMC x nPar) with site parameters Ms inside nPar  of form (npar x nsite) to Ms of the form (nsite x n_par), i.e.  neighboring entries (inside a column) are of the same parameter.\n\nThis format of having n_par as the last dimension helps transforming parameters on block.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.utri2vec-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Internal","title":"HybridVariationalInference.utri2vec","text":"Extract entries of upper diagonal matrix of UppterTriangular to columnwise vector\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.utri2vec_pos-Tuple{Any, Any}","page":"Internal","title":"HybridVariationalInference.utri2vec_pos","text":"Compute the index in the vector of entries in an upper tridiagonal matrix\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.uutri2vec-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Internal","title":"HybridVariationalInference.uutri2vec","text":"Extract entries of upper diagonal matrix of UnitUppterTriangular to columnwise vector\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.vec2utri-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"Internal","title":"HybridVariationalInference.vec2utri","text":"Convert vector v columnwise entries of upper diagonal matrix to UppterTriangular\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_internal/#HybridVariationalInference.vec2utri_pos-Tuple{Any, Any}","page":"Internal","title":"HybridVariationalInference.vec2utri_pos","text":"Compute the (one-based) position (row, col) within an upper tridiagonal matrix for given (one-based) position, s within a packed vector representation.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#Reference","page":"Public","title":"Reference","text":"","category":"section"},{"location":"reference/reference_public/","page":"Public","title":"Public","text":"In this reference, you will find a detailed overview of the package API, i.e. the docstrings.","category":"page"},{"location":"reference/reference_public/#HybridVariationalInference.AbstractComponentArrayInterpreter","page":"Public","title":"HybridVariationalInference.AbstractComponentArrayInterpreter","text":"AbstractComponentArrayInterpreter\n\nInterface for Type that implements\n\nas_ca(::AbstractArray, interpreter) -> ComponentArray\nComponentArrays.getaxes(interpreter)\nBase.length(interpreter) -> Int\n\nWhen called on a vector, forwards to as_ca.\n\nThere is a default implementation for Base.length based on ComponentArrays.getaxes.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.AbstractHybridProblem","page":"Public","title":"HybridVariationalInference.AbstractHybridProblem","text":"Type to dispatch constructing data and network structures for different cases of hybrid problem setups.\n\nFor a specific prob, provide functions that specify details\n\nget_hybridproblem_MLapplicator\nget_hybridproblem_transforms\nget_hybridproblem_PBmodel\nget_hybridproblem_neg_logden_obs\nget_hybridproblem_par_templates\nget_hybridproblem_ϕunc\nget_hybridproblem_train_dataloader (may use construct_dataloader_from_synthetic)\nget_hybridproblem_priors \nget_hybridproblem_n_covar \nget_hybridproblem_n_site_and_batch \n\noptionally\n\ngen_hybridproblem_synthetic\nget_hybridproblem_float_type (defaults to eltype(θM))\nget_hybridproblem_cor_ends (defaults to include all correlations:  (P = [length(θP)], M = [length(θM)]))\nget_hybridproblem_pbmpar_covars (defaults to empty tuple)\n\nThe initial value of parameters to estimate is spread\n\nϕg: parameter of the MLapplicator: returned by get_hybridproblem_MLapplicator\nζP: mean of the PBmodel parameters: returned by get_hybridproblem_par_templates\nϕunc: additional parameters of the approximte posterior: returned by get_hybridproblem_ϕunc\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.AbstractModelApplicator","page":"Public","title":"HybridVariationalInference.AbstractModelApplicator","text":"AbstractModelApplicator(x, ϕ)\n\nAbstraction of applying a machine learning model at covariate matrix, x, using parameters, ϕ. It returns a matrix of predictions with the same number of rows as in x.    \n\nConstructors for specifics are defined in extension packages. Each constructor takes a special type of machine learning model and returns  a tuple with two components:\n\nThe applicator \na sample parameter vector (type  depends on the used ML-framework)\n\nImplemented are\n\nconstruct_SimpleChainsApplicator\nconstruct_FluxApplicator\nconstruct_LuxApplicator\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.AbstractPBMApplicator","page":"Public","title":"HybridVariationalInference.AbstractPBMApplicator","text":"Abstraction of applying a process-based model with  global parameters, θP, site-specific parameters, θMs (sites in columns),  and site-specific model drivers, xP (sites in columns), It returns a matrix of predictions sites in columns.    \n\nSpecific implementations need to provide function apply_model(app, θP, θMs, xP). where\n\nθsP and θsMs are shaped according to the output of generate_ζ, i.e. (n_site_pred x n_par x n_MC).\nResults are of shape (n_obs x n_site_pred x n_MC).\n\nThey may also provide function apply_model(app, θP, θMs, xP) for a sample of parameters, i.e. where an additional dimension is added to both θP and θMs. However, there is a default implementation that mapreduces across these dimensions.\n\nProvided are implementations\n\nPBMSiteApplicator: based on a function that computes predictions per site\nPBMPopulationApplicator: based on a function that computes predictions for entire population\nNullPBMApplicator: returning its input θMs for testing\nPlainPBMApplicator: based on a function that takes the same arguments as apply_model\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.ComponentArrayInterpreter","page":"Public","title":"HybridVariationalInference.ComponentArrayInterpreter","text":"Non-Concrete version of AbstractComponentArrayInterpreter that avoids storing additional type parameters.\n\nDoes not trigger specialization for Interpreters of different axes, but does not allow compiler-inferred length to construct StaticArrays.\n\nUse get_concrete(cai::ComponentArrayInterpreter) to pass a concrete version to  performance-critical functions.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.ComponentArrayInterpreter-Tuple{}","page":"Public","title":"HybridVariationalInference.ComponentArrayInterpreter","text":"ComponentArrayInterpreter(; kwargs...)\nComponentArrayInterpreter(::AbstractComponentArray)\n\nComponentArrayInterpreter(::AbstractComponentArray, n_dims::NTuple{N,<:Integer})\nComponentArrayInterpreter(n_dims::NTuple{N,<:Integer}, ::AbstractComponentArray)\nComponentArrayInterpreter(n_dims::NTuple{N,<:Integer}, ::AbstractComponentArray, m_dims::NTuple{M,<:Integer})\n\nConstruct a ComponentArrayInterpreter <: AbstractComponentArrayInterpreter with components being vectors of given length or given model of a AbstractComponentArray.\n\nThe other constructors allow constructing arrays with additional dimensions.\n\n'''julia     interpreter = ComponentArrayInterpreter(; P=2, M=(2,3), Unc=5)     v = 1.0:length(interpreter)     interpreter(v).M == 2 .+ [1 3 5; 2 4 6]     vm = stack((v,v .* 10, v .* 100))\n\nintm = ComponentArrayInterpreter(interpreter(v), (3,))\nintm(vm)[:Unc, 2]\n\n'''\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.Exp","page":"Public","title":"HybridVariationalInference.Exp","text":"Exp()\n\nA bijector that applies broadcasted exponential function, i.e. exp.(x). It is equivalent to elementwise(exp) but works better with automatic differentiation on GPU.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.HybridProblem","page":"Public","title":"HybridVariationalInference.HybridProblem","text":"Implements AbstractHybridProblem by gathering all the parts into  one struct.\n\nFields:\n\nθP::ComponentVector, θM::ComponentVector: parameter templates\ng::AbstractModelApplicator, ϕg::AbstractVector: ML model and its parameters \nϕunc::ComponentVector: parameters for the Covariance matrix of the approximate posterior\nf_batch: Process-based model predicing for n_batch sites\nf_allsites: Process-based model predicing for n_site sites\npriors: AbstractDict: Prior distributions for all PBM parameters on constrained scale\npy: Likelihood function\ntransM::Stacked, transP::Stacked: bijectors transforming from unconstrained to  constrained scale for site-specific and global parameters respectively.\ntrain_dataloader::MLUtils.DataLoader: providingn Tuple of matrices  (xM, xP, y_o, y_unc, i_sites): covariates, model drivers, observations,  observation uncertainties and index of provided sites.\nn_covar::Int, n_site::Int, n_batch::Int: number covariates, number of sites, and number of sites within one batch\ncor_ends::NamedTuple: block structure in correlations,  defaults to  (P = [length(θP)], M = [length(θM)])\npbm_covars::NTuple{N,Symbol}: names of global parameters used as covariates  in the ML model, defaults to (), i.e. no covariates fed into the ML model\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.HybridProblem-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.HybridProblem","text":"HybridProblem(prob::AbstractHybridProblem; scenario = ()\n\nGather all information from another AbstractHybridProblem with possible updating of some of the entries.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.MagnitudeModelApplicator","page":"Public","title":"HybridVariationalInference.MagnitudeModelApplicator","text":"MagnitudeModelApplicator(app, y0)\n\nWrapper around AbstractModelApplicator that multiplies the prediction of the wrapped app by scalar y0.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.NormalScalingModelApplicator","page":"Public","title":"HybridVariationalInference.NormalScalingModelApplicator","text":"NormalScalingModelApplicator(app, μ, σ)\nNormalScalingModelApplicator(app, priors, transM)\n\nWrapper around AbstractModelApplicator that transforms each output  (assumed in [0..1], such as output of logistic activation function) to a quantile of a Normal distribution. \n\nLength of μ, σ must correspond to the number of outputs of the wrapped ModelApplicator.\n\nThis helps to keep raw ML-predictions (in confidence bounds) and weights in a  similar magnitude. Compared to specifying bounds, this allows for the possibility  (although harder to converge) far beyond the confidence bounds.\n\nThe second constructor fits a normal distribution of the inverse-transformed 5% and 95% quantiles of prior distributions.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.NormalScalingModelApplicator-Tuple{AbstractModelApplicator, Any, Any, Type}","page":"Public","title":"HybridVariationalInference.NormalScalingModelApplicator","text":"NormalScalingModelApplicator(app, lowers, uppers, FT::Type; repeat_inner::Integer = 1)\n\nFit a Normal distribution to number iterators lower and upper and transform  results of the wrapped app AbstractModelApplicator. If repeat_inner is given, each fitted distribution is repeated as many times to support independent multivariate normal distribution.\n\nFT is the specific FloatType to use to construct Distributions,  It usually corresponds to the type used in other ML-parts of the model, e.g. Float32.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.NullModelApplicator","page":"Public","title":"HybridVariationalInference.NullModelApplicator","text":"NullModelApplicator()\n\nModel applicator that returns its inputs. Used for testing.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.NullPBMApplicator","page":"Public","title":"HybridVariationalInference.NullPBMApplicator","text":"NullPBMApplicator()\n\nProcess-Base-Model applicator that returns its θMs inputs. Used for testing.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.PBMPopulationApplicator-Tuple{Any, Any}","page":"Public","title":"HybridVariationalInference.PBMPopulationApplicator","text":"PBMPopulationApplicator(fθpop, n_batch; θP, θM, θFix, xPvec)\n\nConstruct AbstractPBMApplicator from process-based model fθ that computes predictions across sites for a population of size n_batch. The applicator combines enclosed θFix, with provided θMs and θP to a ComponentMatrix with parameters with one row for each site, that can be column-indexed by Symbols.\n\nArguments\n\nfθpop: process model, process model f(θc, xPc), which is agnostic of the partitioning  of parameters into fixed, global, and individual.\nθc: parameters: ComponentMatrix (nsite x npar) with each row a parameter vector\nxPc: observations: ComponentMatrix (nobs x nsite) with each column \nobservationsfor one site\nn_batch: number of indiduals, i.e. rows in θMs\nθP: ComponentVector template of global process model parameters\nθM: ComponentVector template of individual process model parameters\nθFix: ComponentVector of actual fixed process model parameters\nxPvec: ComponentVector template of model drivers for a single site\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.PBMSiteApplicator-Tuple{Any}","page":"Public","title":"HybridVariationalInference.PBMSiteApplicator","text":"PBMSiteApplicator(fθ, n_batch; θP, θM, θFix, xPvec)\n\nConstruct AbstractPBMApplicator from process-based model fθ that computes predictions for a single site. The Applicator combines enclosed θFix, with provided θMs and θP and constructs a ComponentVector that can be indexed by  symbolic parameter names, corresponding to the templates provided during construction of the applicator.\n\nArguments\n\nfθ: process model, process model fθ(θc, xP), which is agnostic of the partitioning\n\nof parameters.\n\nθP: ComponentVector template of global process model parameters\nθM: ComponentVector template of individual process model parameters\nθFix: ComponentVector of actual fixed process model parameters\nxPvec:ComponentVector template of model drivers for a single site\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.StackedArray","page":"Public","title":"HybridVariationalInference.StackedArray","text":"StackedArray(stacked, nrow)\n\nA Bijectors.Transform that applies stacked to each column of an n-row matrix.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.StaticComponentArrayInterpreter","page":"Public","title":"HybridVariationalInference.StaticComponentArrayInterpreter","text":"Concrete version of AbstractComponentArrayInterpreter that stores an axis in its type signature.\n\nAllows compiler-inferred length to construct StaticArrays, but requires specialization on dispatch when provided as an argument to a function.\n\nUse get_concrete(cai::ComponentArrayInterpreter) to pass a concrete version to  performance-critical functions.\n\n\n\n\n\n","category":"type"},{"location":"reference/reference_public/#HybridVariationalInference.apply_preserve_axes-Tuple{Any, ComponentArrays.ComponentArray}","page":"Public","title":"HybridVariationalInference.apply_preserve_axes","text":"apply_preserve_axes(f, ca::ComponentArray)\n\nApply callable f(x) to the data inside ca, assume that the result has the same shape, and return a new ComponentArray with the same axes as in ca.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.callback_loss","page":"Public","title":"HybridVariationalInference.callback_loss","text":"create a function (state, l) -> false that prints iter and loss each moditer\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.compute_correlated_covars-Union{Tuple{T}, Tuple{Random.AbstractRNG, AbstractMatrix{T}}} where T","page":"Public","title":"HybridVariationalInference.compute_correlated_covars","text":"Create n_covar correlated covariates  from uncorrelated row-wise vector x_pc, with correlations rhos to the linear combinations of x_pc.\n\nBy default correlations, rhos = (1.0),0.88,0.78,0.69,0.61 ...,  decrease exponentially as e^{-i/rhodec}, with rhodec = 8.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.construct_3layer_MLApplicator","page":"Public","title":"HybridVariationalInference.construct_3layer_MLApplicator","text":"construct_3layer_MLApplicator(\n    rng::AbstractRNG, prob::HVI.AbstractHybridProblem, <ml_engine>;\n    scenario::Val{scen}) where scen\n\nConstruct a machine learning model for given Proglem and machine learning engine. Implemented for machine learning extensions, such as Flux or SimpleChains. ml_engine usually is of type Val{Symbol}, e.g. Val(:Flux). See select_ml_engine.       \n\nScenario is a value-type of NTuple{_,Symbol}.\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.construct_ChainsApplicator","page":"Public","title":"HybridVariationalInference.construct_ChainsApplicator","text":"construct_ChainsApplicator([rng::AbstractRNG,] chain, float_type)\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.construct_dataloader_from_synthetic-Tuple{Random.AbstractRNG, AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.construct_dataloader_from_synthetic","text":"construct_dataloader_from_synthetic(rng::AbstractRNG, prob::AbstractHybridProblem;\n    scenario = (), n_batch)\n\nConstruct a dataloader based on gen_hybridproblem_synthetic. \n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.construct_partric-Tuple{AbstractModelApplicator, Any, Any}","page":"Public","title":"HybridVariationalInference.construct_partric","text":"Construct a parametric type-stable model applicator, given covariates, x, and parameters, ϕ.\n\nThe default returns the current model applicator.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.cpu_ca-Tuple{ComponentArrays.ComponentArray}","page":"Public","title":"HybridVariationalInference.cpu_ca","text":"cpu_ca(ca::CA.ComponentArray)\n\nMove ComponentArray form gpu to cpu.    \n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.extend_stacked_nrow-Tuple{Bijectors.Stacked, Integer}","page":"Public","title":"HybridVariationalInference.extend_stacked_nrow","text":"extend_stacked_nrow(b::Stacked, nrow::Integer)\n\nCreate a Stacked bijectors that transforms nrow times the elements of the original Stacked bijector.\n\nExample\n\nX = reduce(hcat, ([x + y for x in 0:4 ] for y in 0:10:30))\nb1 = CP.Exp()\nb2 = identity\nb = Stacked((b1,b2), (1:1,2:4))\nbs = extend_stacked_nrow(b, size(X,1))\nXt = reshape(bs(vec(X)), size(X))\n@test Xt[:,1] == b1(X[:,1])\n@test Xt[:,2:4] == b2(X[:,2:4])\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.flatten1-Tuple{ComponentArrays.ComponentVector}","page":"Public","title":"HybridVariationalInference.flatten1","text":"flatten1(cv::CA.ComponentVector)\n\nRemoves the highest level of keys. Keeps the reference to the underlying data, but changes the axis. If first-level vector has no sub-names, an error (Aguement Error tuple must be non-empty) is thrown.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.gdev_hybridproblem_dataloader-Tuple{MLUtils.DataLoader}","page":"Public","title":"HybridVariationalInference.gdev_hybridproblem_dataloader","text":"gdev_hybridproblem_dataloader(dataloader::MLUtils.DataLoader; gdev_M, gdev_P,\n    batchsize = dataloader.batchsize,\n    partial = dataloader.partial\n    )\n\nPut relevant parts of the DataLoader to gpu, depending on scenario.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.gen_cov_pred-Tuple{Random.AbstractRNG, DataType, Any, Any, Any, Integer}","page":"Public","title":"HybridVariationalInference.gen_cov_pred","text":"Generate correlated covariates and synthetic true parameters that are a linear combination of the uncorrelated underlying principal  factors and their binary combinations.\n\nIn addition provide a SimpleChains model of adequate complexity to fit this relationship θMstrue = f(xo)\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.gen_hybridproblem_synthetic","page":"Public","title":"HybridVariationalInference.gen_hybridproblem_synthetic","text":"gen_hybridproblem_synthetic([rng,] ::AbstractHybridProblem; scenario)\n\nSetup synthetic data, a NamedTuple of\n\nxM: matrix of covariates, with one column per site\nθP_true: vector global process-model parameters\nθMs_true: matrix of site-varying process-model parameters, with \nxP: Vector of process-model drivers, with an entry per site\nyglobaltrue: vector of global observations\ny_true: matrix of site-specific observations with one column per site\nyglobalo, y_o: observations with added noise\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_ca_ends-Tuple{ComponentArrays.ComponentVector}","page":"Public","title":"HybridVariationalInference.get_ca_ends","text":"get_ca_ends(vc::ComponentVector)\n\nReturn a Vector with ending positions of components in vc.  Useful for providing information on correlactions among subranges in a vector.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_ca_starts-Tuple{ComponentArrays.ComponentVector}","page":"Public","title":"HybridVariationalInference.get_ca_starts","text":"get_ca_starts(vc::ComponentVector)\n\nReturn a tuple with starting positions of components in vc.  Useful for providing information on correlactions among subranges in a vector.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_cor_count-Tuple{AbstractVector}","page":"Public","title":"HybridVariationalInference.get_cor_count","text":"get_cor_count(cor_ends::AbstractVector)\nget_cor_count(n_par::Integer)\n\nReturn number of correlation coefficients for a correlation matrix of size (npar x npar) With blocks starting a positions given with tuple cor_ends.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_gdev_MP-Union{Tuple{Val{scen}}, Tuple{scen}} where scen","page":"Public","title":"HybridVariationalInference.get_gdev_MP","text":"get_gcdev(scenario::Val{scen}) where scen\n\nConfigure the function that puts data and computations to gpu device  for given scenario. Checking for :use_gpu and :f_on_gpu in scenario. Returns a NamedTuple (;gdev_M, gdev_P)\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_MLapplicator","page":"Public","title":"HybridVariationalInference.get_hybridproblem_MLapplicator","text":"get_hybridproblem_MLapplicator([rng::AbstractRNG,] ::AbstractHybridProblem; scenario=())\n\nConstruct the machine learning model fro given problem prob and ML-Framework and  scenario.\n\nreturns a Tuple of\n\nAbstractModelApplicator\ninitial parameter vector\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_PBmodel","page":"Public","title":"HybridVariationalInference.get_hybridproblem_PBmodel","text":"get_hybridproblem_PBmodel(::AbstractHybridProblem; scenario::NTuple=())\n\nConstruct the process-based model function  f(θP::AbstractVector, θMs::AbstractMatrix, x) -> (AbstractVector, AbstractMatrix) with\n\nθP: calibrated parameters that are constant across site\nθMs: calibrated parameters that vary across sites, with a  column for each site\nx: drivers, indexed by site\n\nreturns a tuple of predictions with components\n\nfirst, those that are constant across sites\nsecond, those that vary across sites, with a column for each site\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_cor_ends-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.get_hybridproblem_cor_ends","text":"get_hybridproblem_cor_ends(prob::AbstractHybridProblem; scenario)\n\nSpecify blocks in correlation matrices among parameters. Returns a NamedTuple.\n\nP: correlations among global parameters\nM: correlations among ML-predicted parameters\n\nSubsets ofparameters that are correlated with other but not correlated with parameters of other subranges are specified by indicating the starting position of each subrange. E.g. if within global parameter vector (p1, p2, p3), p1 and p2 are correlated,  but parameter p3 is not correlated with them, then the first subrange starts at position 1 and the second subrange starts at position 3. If there is only single block of all ML-predicted parameters being correlated  with each other then this block starts at position 1: (P=(1,3), M=(1,)).\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_float_type-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.get_hybridproblem_float_type","text":"get_hybridproblem_float_type(::AbstractHybridProblem; scenario)\n\nDetermine the FloatType for given Case and scenario, defaults to Float32\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_n_covar-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.get_hybridproblem_n_covar","text":"get_hybridproblem_n_covar(::AbstractHybridProblem; scenario)\n\nProvide the number of covariates. \n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_n_site_and_batch","page":"Public","title":"HybridVariationalInference.get_hybridproblem_n_site_and_batch","text":"get_hybridproblem_n_site_and_batch(::AbstractHybridProblem; scenario)\n\nProvide the number of sites. \n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_neg_logden_obs","page":"Public","title":"HybridVariationalInference.get_hybridproblem_neg_logden_obs","text":"get_hybridproblem_neg_logden_obs(::AbstractHybridProblem; scenario)\n\nProvide a function(y_obs, ypred) -> Real that computes the negative logdensity of the observations, given the predictions.\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_par_templates","page":"Public","title":"HybridVariationalInference.get_hybridproblem_par_templates","text":"get_hybridproblem_par_templates(::AbstractHybridProblem; scenario)\n\nProvide tuple of templates of ComponentVectors θP and θM.\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_priors-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.get_hybridproblem_priors","text":"get_hybridproblem_priors(::AbstractHybridProblem; scenario)\n\nReturn a dictionary of marginal prior distributions for components in θP and θM. Defaults for each component θ to Normal(θ, max(θ, 1.0)).\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_train_dataloader","page":"Public","title":"HybridVariationalInference.get_hybridproblem_train_dataloader","text":"get_hybridproblem_train_dataloader(::AbstractHybridProblem; scenario, n_batch)\n\nReturn a DataLoader that provides a tuple of\n\nxM: matrix of covariates, with one column per site\nxP: Iterator of process-model drivers, with one element per site\ny_o: matrix of observations with added noise, with one column per site\ny_unc: matrix sizeof(y_o) of uncertainty information \ni_sites: Vector of indices of sites in the minibatch\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_transforms","page":"Public","title":"HybridVariationalInference.get_hybridproblem_transforms","text":"get_hybridproblem_transforms(::AbstractHybridProblem; scenario)\n\nReturn a NamedTupe of\n\ntransP: Bijectors.Transform for the global PBM parameters, θP\ntransM: Bijectors.Transform for the single-site PBM parameters, θM\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_hybridproblem_ϕunc-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.get_hybridproblem_ϕunc","text":"get_hybridproblem_ϕunc(::AbstractHybridProblem; scenario)\n\nProvide a ComponentArray of the initial additional parameters of the approximate posterior. Defaults to zero correlation and log_σ2 of 1e-10.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_loss_gf","page":"Public","title":"HybridVariationalInference.get_loss_gf","text":"Create a loss function for given\n\ng(x, ϕ): machine learning model \ntransM: transforamtion of parameters at unconstrained space\nf(θMs, θP): mechanistic model \nyoglobal: site-independent observations\nintϕ: interpreter attaching axis with components ϕg and ϕP\nintP: interpreter attaching axis to ζP = ϕP with components used by f\nkwargs: additional keyword arguments passed to gf, such as gdev or pbm_covars\n\nThe loss function loss_gf(ϕ, xM, xP, y_o, y_unc, i_sites) takes   \n\nparameter vector ϕ\nxM: matrix of covariate, sites in the batch are in columns\nxP: iteration of drivers for each site\ny_o: matrix of observations, sites in columns\ny_unc: vector of uncertainty information for each observation\ni_sites: index of sites in the batch\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.get_positions-Tuple{AbstractComponentArrayInterpreter}","page":"Public","title":"HybridVariationalInference.get_positions","text":"get_positions(cai::AbstractComponentArrayInterpreter)\n\nCreate a NamedTuple of integer indices for each component. Assumes that interpreter results in a one-dimensional array, i.e. in a ComponentVector.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.get_quantile_transformed-Tuple{AbstractVector{<:Distributions.Distribution}, Any}","page":"Public","title":"HybridVariationalInference.get_quantile_transformed","text":"Get the inverse-transformation of lower and upper quantiles of a Vector of Distributions.\n\nThis can be used to get proper confidence intervals at unconstrained (log) ζ-scale for priors on normal θ-scale for constructing a NormalScalingModelApplicator.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.gf-Tuple{AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.gf","text":"composition f ∘ transM ∘ g: mechanistic model after machine learning parameter prediction\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.init_hybrid_params-Union{Tuple{FT}, Tuple{AbstractVector{FT}, AbstractVector{FT}, NamedTuple, AbstractVector{FT}, HybridProblemInterpreters}} where FT","page":"Public","title":"HybridVariationalInference.init_hybrid_params","text":"init_hybrid_params(θP, θM, ϕg, n_batch; transP=asℝ, transM=asℝ)\n\nSetup ComponentVector of parameters to optimize, and associated tools. Returns a NamedTuple of\n\nϕ: A ComponentVector of parameters to optimize\ntransPMsbatch, interpreters: Transformations and interpreters as  required by `negelbo_gtf`.\ngettransPMs: a function returning transformations `(nsite) -> (;P,Ms)`\ngetcaintPMs: a function returning ComponentArrayInterpreter for PMs vector  with PMs shaped as a matrix of `nsitecolumns ofθM`\n\nArguments\n\nθP, θM: Template ComponentVectors of global parameters and ML-predicted parameters\ncor_ends: NamedTuple with entries, P, and M, respectively with   integer vectors of ending columns of parameters blocks\nϕg: vector of parameters to optimize, as returned by get_hybridproblem_MLapplicator\nn_batch: the number of sites to predicted in each mini-batch\ntransP, transM: the Bijector.Transformations for the global and site-dependent    parameters, e.g. Stacked(elementwise(identity), elementwise(exp), elementwise(exp)).   Its the transformation froing from unconstrained to constrained space: θ = Tinv(ζ),   because this direction is used much more often.\nϕunc0 initial uncertainty parameters, ComponentVector with format of init_hybrid_ϕunc.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.init_hybrid_ϕunc-Union{Tuple{NamedTuple}, Tuple{FT}, Tuple{NamedTuple, FT}, Tuple{NamedTuple, FT, AbstractVector{FT}}} where FT","page":"Public","title":"HybridVariationalInference.init_hybrid_ϕunc","text":"init_hybrid_ϕunc(cor_ends, ρ0=0f0; logσ2_ζP, coef_logσ2_ζMs, ρsP, ρsM)\n\nInitialize vector of additional parameter of the approximate posterior.\n\nArguments:\n\ncor_ends: NamedTuple with entries, P, and M, respectively with   integer vectors of ending columns of parameters blocks\nρ0: default entry for ρsP and ρsM, defaults = 0f0.\ncoef_logσ2_logM: default column for coef_logσ2_ζMs, defaults to [-10.0, 0.0]\n\nReturns a ComponentVector of \n\nlogσ2_ζP: vector of log-variances of ζP (on log scale). defaults to -10\ncoef_logσ2_ζMs: offset and slope for the log-variances of ζM scaling with   its value given by columns for each parameter in ζM, defaults to [-10, 0]\nρsP and ρsM: parameterization of the upper triangular cholesky factor  of the correlation matrices of ζP and ζM, default to all entries ρ0, which defaults to zero.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.neg_elbo_gtf-Tuple","page":"Public","title":"HybridVariationalInference.neg_elbo_gtf","text":"Cost function (ELBO) for hybrid model with batched sites.\n\nIt generates n_MC samples for each site, and uses these to compute the expected value of the likelihood of observations.\n\nArguments\n\nrng: random number generator (ignored on CUDA, if ϕ is a AbstractGPUArray)\nϕ: flat vector of parameters interpreted by interpreters.μPϕgunc and interpreters.PMs\ng: machine learning model\ntransPMs: Transformations as generated by gettransPMs returned from inithybrid_params\nf: mechanistic model\npy: negative log-likelihood of observations given predictions:  function(y_ob, y_pred, y_unc)\nxM, xP, y_ob, y_unc, i_sites: information of the sites in the current minibatch\nxM: matrix of covariates (ncov x nsite_batch)\nxP: model drivers, iterable of (nsitebatch)\ny_ob: matrix of observations (nobs x nsite_batch)\ny_unc: observation uncertainty provided to py (same size as y_ob)\ni_sites: indices of sites for current minibatch\ninterpreters: NamedTuple as generated by gen_hybridproblem_synthetic with entries:\nμP_ϕg_unc: extract components of parameter of \nmeans of global PBM, 2) ML-weights, and 3) additional parameters of approximation q\nPMs: assign components to PBM parameters 1 global, 2 matrix of n_site column  vectors\nint_unc (can be omitted, if μP_ϕg_unc(ϕ).unc is already a ComponentVector)\nn_MC: number of MonteCarlo samples from the distribution of parameters to simulate using the mechanistic model f.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.neg_logden_indep_normal-Union{Tuple{ET}, Tuple{AbstractArray, AbstractArray, AbstractArray{ET}}} where ET","page":"Public","title":"HybridVariationalInference.neg_logden_indep_normal","text":"neg_logden_indep_normal(obs, μ, logσ2s; σfac=1.0)\n\nCompute the negative Log-density of θM for multiple independent normal distributions, given estimated means μ and estimated log of variance parameters logσ2s.\n\nAll the arguments should be vectors of the same length. If obs,  μ are given as a matrix of several column-vectors, their summed Likelihood is computed, assuming each column having the same logσ2s.\n\nKeyword argument σfac can be increased to put more weight on achieving a low uncertainty estimate and means closer to the observations to help an initial fit. The obtained parameters then can be used as starting values for a the proper fit with σfac=1.0.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.predict_hvi-Tuple{Any, AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.predict_hvi","text":"predict_hvi([rng], predict_hvi(rng, prob::AbstractHybridProblem)\n\nPrediction function for hybrid variational inference parameter model. \n\nArguments\n\nThe problem for which to predict\nxM: covariates for the machine-learning model (ML): Matrix (nθM x nsite_pred).\nxP: model drivers for process based model (PBM): Matrix with (nsitepred) rows. If provided a ComponentArray with a Tuple-Axis in rows, the PBM model can access parts of it, e.g. xP[:S1,...].\n\nKeyword arguments\n\nscenario\nn_sample_pred\n\nReturns an NamedTuple (; y, θsP, θsMs, entropy_ζ) with entries\n\ny: Array (n_obs, n_site, n_sample_pred) of model predictions.\nθsP: ComponentArray (n_θP, n_sample_pred) of PBM model parameters that are kept constant across sites.\nθsMs: ComponentArray (n_site, n_θM, n_sample_pred) of PBM model parameters that vary by site.\nentropy_ζ: The entropy of the log-determinant of the transformation of  the set of model parameters, which is involved in uncertainty quantification.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.sample_posterior-Tuple{Any, AbstractHybridProblem}","page":"Public","title":"HybridVariationalInference.sample_posterior","text":"sample_posterior(rng, prob, [xM::AbstractMatrix]; scenario=Val(()), kwargs...)\n\nSampling the posterior parameter distribution  for hybrid variational inference problem. \n\nArguments\n\nrng: random number generator\nprob: The AbstractHybridProblem from to sample\nxM: covariates for the machine-learning model (ML): Matrix (n_θM x n_site_pred). Default to all sites in get_hybridproblem_train_dataloader(prob; scenario).\n\nOptional keyword arguments    \n\nscenario: scenario to query prob and set default of gpu devices.\nn_sample_pred: number of samples to draw, defaults to 200\ngdevs: NamedTuple(gdev_M, gdev_P): GPU devices for machine learning model  and parameter transformtation, default to get_gdev_MP(scenario).\nis_inferred: set to Val(true) to activate type stabilicy check for transformation\n\nReturns an NamedTuple (; θsP, θsMs, entropy_ζ) with entries\n\nθsP: ComponentArray (n_θP, n_sample_pred) of PBM model parameters that are kept constant across sites.\nθsMs: ComponentArray (n_site, n_θM, n_sample_pred) of PBM model parameters that vary by site.\nentropy_ζ: The entropy of the log-determinant of the transformation of  the set of model parameters, which is involved in uncertainty quantification.\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.scale_centered_at","page":"Public","title":"HybridVariationalInference.scale_centered_at","text":"scale_centered_at(x, m, σrel=1.0)\nscale_centered_at(x, m, σ)\n\nCenters and rescales rows of matrix x around vector m. The scale can either be given relative to m or specified as a vector of same size as m.\n\n\n\n\n\n","category":"function"},{"location":"reference/reference_public/#HybridVariationalInference.select_ml_engine-Union{Tuple{}, Tuple{scen}} where scen","page":"Public","title":"HybridVariationalInference.select_ml_engine","text":"select_ml_engine(;scenario)\n\nReturns a value type Val{:Symbol} to dispatch on the machine learning engine to use.\n\ndefaults to Val(:SimpleChains)\n:use_Lux ∈ scenario -> Val(:Lux)\n:use_Flux ∈ scenario -> Val(:Flux)\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.stack_ca_int-Union{Tuple{n_dims}, Tuple{IT}, Tuple{IT, Val{n_dims}}} where {IT<:AbstractComponentArrayInterpreter, n_dims}","page":"Public","title":"HybridVariationalInference.stack_ca_int","text":"stack_ca_int(cai::AbstractComponentArrayInterpreter, ::Val{n_dims})\n\nInterpret the first dimension of an Array as a ComponentArray. Provide the Tuple of following dimensions by a value type, e.g. Val((n_col, n_z)).\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.DoubleMM.f_doubleMM-Union{Tuple{ET}, Tuple{ComponentArrays.ComponentVector{ET}, Any}} where ET","page":"Public","title":"HybridVariationalInference.DoubleMM.f_doubleMM","text":"f_doubleMM(θc::CA.ComponentVector{ET}, x) where ET\n\nExample process based model (PBM) predicts a double-monod constrained rate for different substrate concentration vectors, x.S1, and x.S2 for a single site. θc is a ComponentVector with scalar parameters as components: r0, r1, K1, and K2\n\nIt predicts a rate for each entry in concentrations: y = r0 .+ r1 .* x.S1 ./ (K1 .+ x.S1) .* x.S2 ./ (K2 .+ x.S2).\n\nIt is defined as \n\nfunction f_doubleMM(θc::ComponentVector{ET}, x) where ET\n    # extract parameters not depending on order, i.e whether they are in θP or θM\n    # r0 = θc[:r0]\n    (r0, r1, K1, K2) = map((:r0, :r1, :K1, :K2)) do par\n        getdata(θc[par])::ET\n    end\n    y = r0 .+ r1 .* x.S1 ./ (K1 .+ x.S1) .* x.S2 ./ (K2 .+ x.S2)\n    return (y)\nend\n\n\n\n\n\n","category":"method"},{"location":"reference/reference_public/#HybridVariationalInference.DoubleMM.f_doubleMM_sites-Tuple{ComponentArrays.ComponentMatrix, ComponentArrays.ComponentMatrix}","page":"Public","title":"HybridVariationalInference.DoubleMM.f_doubleMM_sites","text":"f_doubleMM_sites(θc::CA.ComponentMatrix, xPc::CA.ComponentMatrix)\n\nExample process based model (PBM) that predicts for a batch of sites.\n\nArguments\n\nθc: parameters with one row per site and symbolic column index \nxPc: model drivers with one column per site, and symbolic row index\n\nReturns a matrix (n_obs x n_site) of predictions.\n\nfunction f_doubleMM_sites(θc::ComponentMatrix, xPc::ComponentMatrix)\n    # extract several covariates from xP\n    ST = typeof(getdata(xPc)[1:1,:])  # workaround for non-type-stable Symbol-indexing\n    S1 = (getdata(xPc[:S1,:])::ST)   \n    S2 = (getdata(xPc[:S2,:])::ST)\n    #\n    # extract the parameters as vectors that are row-repeated into a matrix\n    n_obs = size(S1, 1)\n    VT = typeof(getdata(θc)[:,1])   # workaround for non-type-stable Symbol-indexing\n    (r0, r1, K1, K2) = map((:r0, :r1, :K1, :K2)) do par\n        p1 = getdata(θc[:, par]) ::VT\n        repeat(p1', n_obs)  # matrix: same for each concentration row in S1\n    end\n    #\n    # each variable is a matrix (n_obs x n_site)\n    r0 .+ r1 .* S1 ./ (K1 .+ S1) .* S2 ./ (K2 .+ S2)\nend\n\n\n\n\n\n","category":"method"}]
}
