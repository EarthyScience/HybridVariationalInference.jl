"""
Cost function (ELBO) for hybrid model with batched sites.

It generates n_MC samples for each site, and uses these to compute the
expected value of the likelihood of observations.

## Arguments
- `rng`: random number generator (ignored on CUDA, if ϕ is a AbstractGPUArray)
- `ϕ`: flat vector of parameters
  interpreted by interpreters.μP_ϕg_unc and interpreters.PMs
- `g`: machine learning model
- `transPMs`: Transformations as generated by get_transPMs returned from init_hybrid_params
- `f`: mechanistic model
- `py`: negative log-likelihood of observations given predictions: 
  `function(y_ob, y_pred, y_unc)`
- `xM`, `xP`, `y_ob`, `y_unc`, `i_sites`: information of the sites in the current minibatch
    - `xM`: matrix of covariates (n_cov x n_site_batch)
    - `xP`: model drivers, iterable of (n_site_batch)
    - `y_ob`: matrix of observations (n_obs x n_site_batch)
    - `y_unc`: observation uncertainty provided to py (same size as y_ob)
    - `i_sites`: indices of sites for current minibatch
- `interpreters`: NamedTuple as generated by `gen_hybridproblem_synthetic` with entries:
  - `μP_ϕg_unc`: extract components of parameter of 
    1) means of global PBM, 2) ML-weights, and 3) additional parameters of approximation q
  - `PMs`: assign components to PBM parameters 1 global, 2 matrix of n_site column  vectors
  - `int_unc` (can be omitted, if `μP_ϕg_unc(ϕ).unc` is already a ComponentVector)
- `n_MC`: number of MonteCarlo samples from the distribution of parameters to simulate
  using the mechanistic model f.
"""
function neg_elbo_gtf(args...; kwargs...)
    nLy, entropy_ζ, nLmean_θ = neg_elbo_gtf_components(args...; kwargs...)
    nLy - entropy_ζ + nLmean_θ
end

function neg_elbo_gtf_components(rng, ϕ::AbstractVector, g, transPMs, f, py,
        xM::AbstractMatrix, xP, y_ob, y_unc, i_sites::AbstractVector{<:Number},
        interpreters::NamedTuple;
        n_MC = 12, n_MC_mean = 30, n_MC_cap = n_MC,
        cdev = cpu_device(),
        priors_θ_mean = [],
        cor_ends # =(P=(1,),M=(1,))
)
    n_MCr = isempty(priors_θ_mean) ? n_MC : max(n_MC, n_MC_mean)
    ζs, σ = generate_ζ(rng, g, ϕ, xM, interpreters; n_MC = n_MCr, cor_ends)
    ζs_cpu = cdev(ζs) # differentiable fetch to CPU in Flux package extension
    nLy, entropy_ζ = neg_elbo_ζtf(ζs_cpu, σ, transPMs, f, py,
        xP, y_ob, y_unc, interpreters;
        n_MC, n_MC_cap
    )
    nLmean_θ = isempty(priors_θ_mean) ? 0.0 :
    begin
        # compute the mean of predicted and transformed site-parameters
        # avoid mapslices because of Zygote
        # θs0 = mapslices(transPMs, ζs_cpu, dims=[1])
        # θPs0 = mapslices(θ -> interpreters.PMs(θ).P, θs0, dims = 1) 
        #θs = (transPMs(ζ) for ζ in eachcol(ζs_cpu)) # does not work with Zygote
        θs = map(transPMs, eachcol(ζs_cpu))
        θPs = map(θ -> CA.getdata(interpreters.PMs(θ).P), θs) |> stack
        # does not need to allocate vectors but does not work with Zygote: 
        # θPs = (CA.getdata(interpreters.PMs(θ).P) for θ in θs) |> stack
        mean_θP = mean(CA.getdata(θPs); dims = (2))[:, 1]
        #nLmean_θP = map((d, θi) -> -logpdf(d, θi), CA.getdata(priors_θ_mean.P), mean_θP) 
        #workaround for Zygote failing on `priors_θ_mean.P`
        iθ = CA.ComponentArray(1:length(priors_θ_mean), CA.getaxes(priors_θ_mean))
        nLmean_θP = map((d, θi) -> -logpdf(d, θi), priors_θ_mean[CA.getdata(iθ.P)], mean_θP)
        θMss = map(θ -> interpreters.PMs(θ).Ms, θs) |> stack
        mean_θMs = mean(θMss; dims = (3))[:, :, 1]
        nLmean_θMs = map((d, θi) -> -logpdf(d, θi),
            CA.getdata(priors_θ_mean[CA.getdata(iθ.Ms)])[:, i_sites], mean_θMs)
        nLmean_θ = sum(nLmean_θP) + sum(nLmean_θMs)
    end
    nLy, entropy_ζ, nLmean_θ
end

function neg_elbo_ζtf(ζs, σ, transPMs, f, py,
        xP, y_ob, y_unc, interpreters::NamedTuple;
        n_MC = 12, n_MC_cap = n_MC
)
    nLys = map(eachcol(ζs[:, 1:n_MC])) do ζi
        θ_i, y_pred_i, logjac = predict_y(ζi, xP, f, transPMs, interpreters.PMs)
        # TODO nLogDen prior on \theta
        #nLy1 = neg_logden_indep_normal(y_ob, y_pred_i, y_unc)
        nLy1 = py(y_ob, y_pred_i, y_unc)
        nLy1 - logjac
    end
    # For robustness may compute the expectation only on the  n_smallest values
    # because its very sensitive to few large outliers
    #nLys_smallest = nsmallest(n_MC_cap, nLys) # does not work with Zygote
    if n_MC_cap == n_MC
        nLy = sum(nLys) / n_MC
    else
        nLys_smallest = partialsort(nLys, 1:n_MC_cap)
        nLy = sum(nLys_smallest) / n_MC_cap
    end
    #  sum_log_σ = sum(log.(σ))
    # logdet_jacT2 = -sum_log_σ # log Prod(1/σ_i) = -sum log σ_i 
    logdetΣ = 2 * sum(log.(σ))
    entropy_ζ = entropy_MvNormal(size(ζs, 1), logdetΣ)  # defined in logden_normal
    # if i_sites[1] == 1
    #     #Main.@infiltrate_main
    #     @show nLy, entropy_ζ, nLmean_θ, n_MC, n_MC_cap, i_sites[1:3]
    #     @show std(nLys), std(nLys)/abs(nLy)
    #     @show std(nLys_smallest), std(nLys_smallest)/abs(nLy)
    # end
    nLy, entropy_ζ
end

() -> begin
    nLy = reduce(
        +, map(eachcol(ζs_cpu[:, 1:n_MC])) do ζi
            θ_i, y_pred_i, logjac = predict_y(ζi, xP, f, transPMs, interpreters.PMs)
            # TODO nLogDen prior on \theta
            #nLy1 = neg_logden_indep_normal(y_ob, y_pred_i, y_unc)
            nLy1 = py(y_ob, y_pred_i, y_unc)
            nLy1 - logjac
        end) / n_MC
end

() -> begin
    # using UnicodePlots
    histogram(nLys)
end

"""
    predict_gf(rng, g, f, ϕ::AbstractVector, xM::AbstractMatrix, interpreters;
        get_transPMs, get_ca_int_PMs, n_sample_pred=200, gdev = identity)

Prediction function for hybrid model. Returns an NamedTuple with entries
- `θ`: ComponentArray `(n_θP + n_site * n_θM), n_sample_pred)` of PBM model parameters.
- `y`: Array `(n_obs, n_site, n_sample_pred)` of model predictions.
"""
function predict_gf(rng, prob::AbstractHybridProblem, xM::AbstractMatrix, xP;
        scenario,
        n_sample_pred = 200,
        gdev = :use_gpu ∈ scenario ? gpu_device() : identity, 
        cdev = gdev isa MLDataDevices.AbstractGPUDevice ? cpu_device() : identity,
)
    n_site = length(xP)
    @assert size(xM, 2) == n_site
    par_templates = get_hybridproblem_par_templates(prob; scenario)
    (; θP, θM) = par_templates
    cor_ends = get_hybridproblem_cor_ends(prob; scenario)
    g, ϕg0 = get_hybridproblem_MLapplicator(prob; scenario)
    ϕunc0 = get_hybridproblem_ϕunc(prob; scenario)
    (; transP, transM) = get_hybridproblem_transforms(prob; scenario)
    f = get_hybridproblem_PBmodel(prob; scenario)
    (; ϕ, transPMs_batch, interpreters, get_transPMs, get_ca_int_PMs) = init_hybrid_params(
        θP, θM, cor_ends, ϕg0, n_site; transP, transM, ϕunc0)
    g_dev, ϕ_dev = gdev(g), gdev(ϕ)
    predict_gf(rng, g_dev, f, ϕ_dev, xM, xP, interpreters;
        get_transPMs, get_ca_int_PMs, n_sample_pred, cdev, cor_ends)
end

function predict_gf(rng, g, f, ϕ::AbstractVector, xM::AbstractMatrix, xP, interpreters;
        get_transPMs, get_ca_int_PMs, n_sample_pred = 200,
        cdev = cpu_device(),
        cor_ends        #cor_ends=(P=(1,),M=(1,))
)
    n_site = size(xM, 2)
    intm_PMs_gen = get_ca_int_PMs(n_site)
    trans_PMs_gen = get_transPMs(n_site)
    interpreters_gen = (; interpreters..., PMs = intm_PMs_gen)
    ζs_gpu, σ = generate_ζ(rng, g, CA.getdata(ϕ), CA.getdata(xM), interpreters_gen;
        n_MC = n_sample_pred, cor_ends)
    ζs = cdev(ζs_gpu)
    logdetΣ = 2 * sum(log.(σ))
    entropy_ζ = entropy_MvNormal(length(σ), logdetΣ)  # defined in logden_normal
    #y_pred_global, y_pred = f(θc.P, θc.Ms, xP)
    # TODO take care of y_pred_global
    θandy = map(eachcol(ζs)) do ζ
        predict_y(ζ, xP, f, trans_PMs_gen, interpreters_gen.PMs)[1:2]
    end
    θ1 = first(first(θandy))
    θ = CA.ComponentMatrix(
        stack(CA.getdata.(first.(θandy))), (CA.getaxes(θ1)[1], CA.FlatAxis()))
    #θ[:P,1]
    y = stack(last.(θandy))
    (; θ, y, entropy_ζ)
end

"""
Generate samples of (inv-transformed) model parameters, ζ, 
and the vector of standard deviations, σ, i.e. the diagonal of the cholesky-factor.

Adds the MV-normally distributed residuals, retrieved by `sample_ζ_norm0`
to the means extracted from parameters and predicted by the machine learning
model. 
"""
function generate_ζ(rng, g, ϕ::AbstractVector, xM::AbstractMatrix,
        interpreters::NamedTuple; n_MC = 3, cor_ends)
    # see documentation of neg_elbo_gtf
    ϕc = interpreters.μP_ϕg_unc(CA.getdata(ϕ))
    μ_ζP = ϕc.μP
    ϕg = ϕc.ϕg
    μ_ζMs0 = g(xM, ϕg) # TODO provide μ_ζP to g
    ζ_resid, σ = sample_ζ_norm0(rng, μ_ζP, μ_ζMs0, ϕc.unc; n_MC, cor_ends)
    #ζ_resid, σ = sample_ζ_norm0(rng, ϕ[1:2], reshape(ϕ[2 .+ (1:20)],2,:), ϕ[(end-length(interpreters.unc)+1):end], interpreters.unc; n_MC)
    # @show size(ζ_resid)
    # @show length(interpreters.PMs)
    ζ = stack(map(eachcol(ζ_resid)) do r
        rc = interpreters.PMs(r)
        ζP = μ_ζP .+ rc.P
        μ_ζMs = μ_ζMs0 # g(xM, ϕc.ϕ) # TODO provide ζP to g
        ζMs = μ_ζMs .+ rc.Ms
        vcat(ζP, vec(ζMs))
    end)
    ζ, σ
end

"""
Extract relevant parameters from θ and return n_MC generated draws
together with the vector of standard deviations, σ.

## Arguments
`int_unc`: Interpret vector as ComponentVector with components
   ρsP, ρsM, logσ2_logP, coef_logσ2_logMs(intercept + slope), 
"""
function sample_ζ_norm0(rng::Random.AbstractRNG, ζP::AbstractVector, ζMs::AbstractMatrix,
        args...; n_MC, cor_ends)
    n_θP, n_θMs = length(ζP), length(ζMs)
    urand = _create_random(rng, CA.getdata(ζP), n_θP + n_θMs, n_MC)
    sample_ζ_norm0(urand, ζP, ζMs, args...; cor_ends)
end

function sample_ζ_norm0(urand::AbstractMatrix, ζP::AbstractVector{T}, ζMs::AbstractMatrix,
        ϕunc::AbstractVector, int_unc = ComponentArrayInterpreter(ϕunc); cor_ends
) where {T}
    ϕuncc = int_unc(CA.getdata(ϕunc))
    n_θP, n_θMs, (n_θM, n_batch) = length(ζP), length(ζMs), size(ζMs)
    # do not create a UpperTriangular Matrix of an AbstractGÜUArray in transformU_cholesky1
    ρsP = isempty(ϕuncc.ρsP) ? similar(ϕuncc.ρsP) : ϕuncc.ρsP # required by zygote
    UP = transformU_block_cholesky1(ρsP, cor_ends.P)
    ρsM = isempty(ϕuncc.ρsM) ? similar(ϕuncc.ρsM) : ϕuncc.ρsM # required by zygote
    UM = transformU_block_cholesky1(ρsM, cor_ends.M)
    cf = ϕuncc.coef_logσ2_logMs
    logσ2_logMs = vec(cf[1, :] .+ cf[2, :] .* ζMs)
    logσ2_logP = vec(CA.getdata(ϕuncc.logσ2_logP))
    # CUDA cannot multiply BlockDiagonal * Diagonal, construct already those blocks
    σMs = reshape(exp.(logσ2_logMs ./ 2), n_θM, :)
    σP = exp.(logσ2_logP ./ 2)
    # BlockDiagonal does work with CUDA, but not with combination of Zygote and CUDA
    # need to construct full matrix for CUDA
    Uσ = _create_blockdiag(UP, UM, σP, σMs, n_batch)
    ζ_resid = Uσ' * urand
    σ = diag(Uσ)   # elements of the diagonal: standard deviations
    # returns AbstractGPUuArrays to either continue on GPU or need to transfer to CPU
    ζ_resid, σ
end

function _create_blockdiag(UP::AbstractMatrix{T}, UM, σP, σMs, n_batch) where {T}
    v = [i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch]
    #Main.@infiltrate_main
    BlockDiagonal(v)
end
function _create_blockdiag(
        UP::GPUArraysCore.AbstractGPUMatrix{T}, UM, σP, σMs, n_batch) where {T}
    # using BlockDiagonal leads to Scalar operations downstream
    # v = [i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch]
    # BlockDiagonal(v)    
    # Uσ = cat([i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch]...;
    #     dims=(1, 2))
    # on GPU use only one big multiplication rather than many small ones
    U = cat([i == 0 ? UP : UM for i in 0:n_batch]...; dims = (1, 2))
    #Main.@infiltrate_main
    σD = Diagonal(vcat(σP, vec(σMs)))
    Uσ = U * σD
    # need for Zygote why?
    # tmp = cat(Uσ; dims=(1,2))
    tmp = vcat(Uσ)
end

function _create_random(rng, ::AbstractVector{T}, dims...) where {T}
    rand(rng, T, dims...)
end

#moved to HybridVariationalInferenceCUDAExt
#function _create_random(rng, ::CUDA.CuVector{T}, dims...) where {T}


""" 
Compute predictions and log-Determinant of the transformation at given
transformed parameters for each site. 

The number of sites is given by the number of columns in `Ms`, which is determined
by the transformation, `transPMs`.

Steps:
- transform the parameters to original constrained space
- Applies the mechanistic model for each site
"""
function predict_y(ζi, xP, f, transPMs::Bijectors.Transform,
        int_PMs::AbstractComponentArrayInterpreter)
    θc, logjac = transform_ζ(ζi, transPMs, int_PMs)
    y_pred_global, y_pred = f(θc.P, θc.Ms, xP)
    # TODO take care of y_pred_global
    θc, y_pred, logjac
end

function transform_ζ(ζi, transPMs::Bijectors.Transform,
        int_PMs::AbstractComponentArrayInterpreter)
    # θtup, logjac = transform_and_logjac(transPMs, ζi) # both allocating
    # θc = CA.ComponentVector(θtup)
    #replace with more flexible transPMs after trying CUDA/Zygote            
    #θ, logjac = exp.(ζi), sum(ζi)
    θ, logjac = Bijectors.with_logabsdet_jacobian(transPMs, ζi) # both allocating
    θc = int_PMs(θ)
    θc, logjac
end
