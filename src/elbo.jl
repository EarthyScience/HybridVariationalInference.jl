"""
Cost function (ELBO) for hybrid model with batched sites.

It generates n_MC samples for each site, and uses these to compute the
expected value of the likelihood of observations.

## Arguments
- `rng`: random number generator (ignored on CUDA, if ϕ is a AbstractGPUArray)
- `ϕ`: flat vector of parameters
  interpreted by interpreters.ϕg_ϕq and interpreters.PMs
- `g`: machine learning model
- `transPMs`: Transformations as generated by get_transPMs returned from init_hybrid_params
- `f`: mechanistic model
- `py`: negative log-likelihood of observations given predictions: 
  `function(y_ob, y_pred, y_unc)`
- `xM`, `xP`, `y_ob`, `y_unc`, `i_sites`: information of the sites in the current minibatch
    - `xM`: matrix of covariates (n_cov x n_site_batch)
    - `xP`: model drivers, iterable of (n_site_batch)
    - `y_ob`: matrix of observations (n_obs x n_site_batch)
    - `y_unc`: observation uncertainty provided to py (same size as y_ob)
    - `i_sites`: indices of sites for current minibatch
- `interpreters`: NamedTuple as generated by `gen_hybridproblem_synthetic` with entries:
  - `ϕg_ϕq`: extract components of parameter of 
    1) means of global PBM, 2) ML-weights, and 3) additional parameters of approximation q
  - `PMs`: assign components to PBM parameters 1 global, 2 matrix of n_site column  vectors
  - `int_ϕq` interpreter of components of ϕq
- `n_MC`: number of MonteCarlo samples from the distribution of parameters to simulate
  using the mechanistic model f.
"""
function neg_elbo_gtf(args...; kwargs...)
    # TODO prior and penalty loss
    (;nLjoint, entropy_ζ, loss_penalty, 
        nLy, neg_log_prior, neg_log_jac, 
        #nLmean_θ
        ) = neg_elbo_gtf_components(args...; kwargs...)
    nL = nLjoint - entropy_ζ + loss_penalty #+ nLmean_θ
    # if !isfinite(nL) 
    #     @show nL
    #     @show nLjoint, entropy_ζ, loss_penalty, nLy, 
    #     @show neg_log_prior, neg_log_jac, nLmean_θ
    #     Main.@infiltrate_main
    # end
    return(nL)
end

function neg_elbo_gtf_components(rng, ϕ::AbstractVector{FT}, g, f, py,
    xM::AbstractMatrix, xP, y_ob, y_unc, i_sites::AbstractVector{<:Number};
    int_ϕg_ϕq::AbstractComponentArrayInterpreter,
    int_ϕq::AbstractComponentArrayInterpreter,
    n_MC=12, n_MC_mean=n_MC, n_MC_cap=n_MC,
    cdev=cpu_device(),
    priors_θP_mean=[], 
    priors_θMs_mean=[],
    #priors_θ_mean=[],
    cor_ends, # =(P=(1,),M=(1,))
    pbm_covar_indices,
    transP, transMs, 
    trans_mP =StackedArray(transP, n_MC), # provide with creating cost function
    trans_mMs =StackedArray(transMs.stacked, n_MC),
    priorsP, priorsM,
    floss_penalty = zero_penalty_loss,
    is_testmode,
    is_omit_priors,
    zero_prior_logdensity,
    approx::AbstractHVIApproximation,
) where {FT}
    n_MCr = isempty(priors_θP_mean) ? n_MC : max(n_MC, n_MC_mean)
    ζsP, ζsMs, σ = generate_ζ(approx, rng, g, ϕ, xM; n_MC=n_MCr, cor_ends, pbm_covar_indices,
        int_ϕq, int_ϕg_ϕq, is_testmode)
    ζsP_cpu = cdev(ζsP) # fetch to CPU, because for <1000 sites (n_batch) this is faster
    ζsMs_cpu = cdev(ζsMs) # fetch to CPU, because for <1000 sites (n_batch) this is faster
    #
    # maybe: translate ζ once and supply to both neg_elbo and negloglik_meanθ
    ϕc = int_ϕg_ϕq(ϕ)
    VT= typeof(@view(ϕ[1:1]))
    ϕg = CA.getdata(ϕc.ϕg)::VT
    ϕq = CA.getdata(ϕc.ϕq)::VT
    loss_comps = neg_elbo_ζtf(
        ζsP_cpu[:,1:n_MC], ζsMs_cpu[:,:,1:n_MC], σ, f, py, xP, y_ob, y_unc;
        n_MC_cap, transP, transMs, priorsP, priorsM, 
        floss_penalty, ϕg, ϕq, is_omit_priors, zero_prior_logdensity,)
    #
    # maybe: provide trans_mP and trans_mMs with creating cost function
    # not used any more and merging named tuples takes long
    # nLmean_θ = _compute_negloglik_meanθ(ζsP_cpu, ζsMs_cpu; 
    #     trans_mP, trans_mMs, priors_θP_mean, priors_θMs_mean, i_sites, )
    # (;loss_comps..., nLmean_θ)
end

function _compute_negloglik_meanθ(ζsP::AbstractMatrix{FT}, ζsMs; 
    priors_θP_mean, priors_θMs_mean, i_sites, trans_mP, trans_mMs, 
) where FT
    if isempty(priors_θP_mean) 
        return zero(FT)
    end
    θsP, θsMs = transform_ζs(ζsP, ζsMs; trans_mP, trans_mMs)
    mean_θP = mean(CA.getdata(θsP); dims=(2))[:, 1]
    nLmean_θP = map((d, θi) -> -logpdf(d, θi), priors_θP_mean, mean_θP)
    mean_θMs = mean(θsMs; dims=(3))[:, :, 1]
    nLmean_θMs = map((d, θi) -> -logpdf(d, θi), priors_θMs_mean[i_sites], mean_θMs)
    nLmean_θ = sum(nLmean_θP) + sum(nLmean_θMs)
    convert(FT,nLmean_θ)::FT
end

"""
    get_zero_prior_logdensity(priorsP::Tuple, priorsM::Tuple, θP, θM)

invoke logpdf of prior and sum, to infer return type and proper zero of prior density.
"""
function get_zero_prior_logdensity(priorsP::Tuple, priorsM::Tuple, θP, θM)
    zd = get_zero_prior_logdensity(priorsM, θM) 
    isempty(priorsP) ? zd : zd + get_zero_prior_logdensity(priorsP, θP) 
end
function get_zero_prior_logdensity(priors::Tuple, θ)
    logpdf_t = (prior, θ) -> logpdf(prior, θ)
    zero(sum(map(logpdf_t, priors, θ)))
end

""" 
Compute the neg_elbo for each sampled parameter vector (last dimension of ζs).
- Transform and compute log-jac
- call forward model
- compute log-density of joint density of predictions and unconstrained parameters, `nLjoint`
  and its components
  - `nLy`: The likelihood of the data, given the parameters
  - `neg_log_prior`: the prior of parameters at constrained scale
  - `logjac`, negative logarithm of the absolute value of the determinant of the Jacobian of 
    the transformation `θ=T(ζ)`.
- `loss_penalty`: additional loss terms from floss_penalty
- compute entropy of transformation
"""
function neg_elbo_ζtf(ζsP, ζsMs, σ, f, py, xP, y_ob, y_unc;
    n_MC_cap=size(ζsP,2),
    transP,
    transMs=StackedArray(transM, size(ζsMs, 2)),
    priorsP, priorsM,
    floss_penalty, ϕg, ϕq,
    is_omit_priors::Val,
    zero_prior_logdensity,
) 
    n_MC = size(ζsP,2)
    f_sample = (ζP, ζMs) -> begin    
            θP, θMs, logjac_i = transform_and_logjac_ζ(ζP, ζMs; transP, transMs)
            #  currently logpdf only works on CPU
            y_pred_i = f(θP, θMs, xP)
            #nLy1 = neg_logden_indep_normal(y_ob, y_pred_i, y_unc)
            # Main.@infiltrate_main
            # Test.@inferred( f(θP, θMs, xP) )
            # using ShareAdd
            # @usingany Cthulhu
            # @descend_code_warntype f(θP, θMs, xP)
            nLy_i = py(y_ob, y_pred_i, y_unc)
            loss_penalty_i = convert(eltype(nLy_i),floss_penalty(y_pred_i, θMs, θP, ϕg, ϕq))
            neg_log_prior_i = compute_priors_logdensity(priorsP, priorsM, θP, θMs,
                is_omit_priors, zero_prior_logdensity)
            # make sure names to not match outer, otherwise Box type instability
            (nLy_i, neg_log_prior_i, -logjac_i, loss_penalty_i)
            #(nLy_i, 0.0, 0.0, 0.0)
        end
    # only Vector inferred, need to provide type hint
    # make that all components use the same Float type
    # Test.@inferred f_sample(first(eachcol(ζsP)), first(eachslice(ζsMs; dims=3)))
    # Test.@inferred map(f_sample, eachcol(ζsP), eachslice(ζsMs; dims=3))
    #using ShareAdd
    #@usingany Cthulhu
    #@descend_code_warntype f_sample(first(eachcol(ζsP)), first(eachslice(ζsMs; dims=3)))
    #map_res = Test.@inferred map(f_sample, eachcol(ζsP), eachslice(ζsMs; dims=3))
    map_res = map(f_sample, eachcol(ζsP), eachslice(ζsMs; dims=3))
    nLys, neg_log_priors, neglogjacs, loss_penalties = vectuptotupvec(map_res)
    # For robustness may compute the expectation only on the n_smallest values
    # because its very sensitive to few large outliers
    #nLys_smallest = nsmallest(n_MC_cap, nLys) # does not work with Zygote
    if n_MC_cap == n_MC
        nLy = sum(nLys) / n_MC
        neg_log_prior = sum(neg_log_priors) / n_MC
        neg_log_jac = sum(neglogjacs) / n_MC
        loss_penalty = sum(loss_penalties) / n_MC
    else
        @warn "neg_elbo_ζtf: TPDP n_MC_cap: implement for for logjac, loss_penalty, and neg_log_prior not capped"
        nLys_smallest = partialsort(nLys, 1:n_MC_cap)
        nLy = sum(nLys_smallest) / n_MC_cap
    end
    #  sum_log_σ = sum(log.(σ))
    # logdet_jacT2 = -sum_log_σ # log Prod(1/σ_i) = -sum log σ_i 
    logdetΣ = 2 * sum(log.(σ))
    n_θ = size(ζsP, 1) + prod(size(ζsMs)[1:2])
    if length(σ) != n_θ
        error("TODO infiltrate")
        #Main.@infiltrate_main
    end
    #@assert length(σ) == n_θ
    entropy_ζ = entropy_MvNormal(n_θ, logdetΣ)  # defined in logden_normal
    # if i_sites[1] == 1
    #     #Main.@infiltrate_main
    #     @show nLy, entropy_ζ, nLmean_θ, n_MC, n_MC_cap, i_sites[1:3]
    #     @show std(nLys), std(nLys)/abs(nLy)
    #     @show std(nLys_smallest), std(nLys_smallest)/abs(nLy)
    # end
    nLjoint = nLy + neg_log_prior + neg_log_jac
    (;nLjoint, entropy_ζ, loss_penalty, nLy, neg_log_prior, neg_log_jac)
end

function compute_priors_logdensity(priorsP, priorsM, θP, θMs,
        ::Val{omit_priors}, zero_prior_logdensity) where {omit_priors}
    if omit_priors
        zero_prior_logdensity
    elseif (θP isa AbstractGPUArray) || (θMs isa AbstractGPUArray)
        @warn("neg_elbo_ζtf: Cannot apply priors to gpu array. Piors are omitted. "*
        "either compute PBM on CPU or omit priors.")
        zero_prior_logdensity
    else
        compute_priors_logdensity(priorsP, priorsM, θP, θMs, zero_prior_logdensity)
    end
end

function compute_priors_logdensity(priorsP, priorsM, θP, θMs, zero_prior_logdensity)
    logpdf_t = (prior, θ) -> logpdf(prior, θ)::eltype(θP)
    function logpdf_tv_sum(prior, θ::AbstractVector{T}) where T 
        # logpdf_tv_sum_inner = let prior = prior
        #     function(θi)
        #         lp = logpdf(prior, θi)
        #         # TT = ChainRulesCore.@ignore_derivatives Base.return_types(logpdf, Tuple{typeof(prior), typeof(θi)})
        #         # if TT != [typeof(lp)] 
        #         #     error("encountered unstable logpdf: $TT")
        #         # end
        #         lp
        #     end
        # end
        # sum(logpdf_tv_sum_inner, θ)
        sum(θi -> logpdf(prior, θi), θ)
    end
    # handle edge case of no global parameters, where priorsP is empty
    nlP0 = isempty(priorsP) ? zero_prior_logdensity : -sum(logpdf_t.(priorsP, θP))
    # fi = (priorMi, θMi) -> begin
    #     logpdf_tv_sum(priorMi, θMi)
    #     sum(logpdf_tv_sum(priorMi, θMi))::eltype(θMi)
    # end
    f_col = let priorsM=priorsM, θMs=θMs
        function f_col_inner(i)
            # TP = ChainRulesCore.@ignore_derivatives Base.return_types(getindex, Tuple{typeof(priorsM), typeof(i)})
            # if TP != [typeof(priorsM[i])] 
            #     error("encountered unstable priorsM: $TP")
            # end
            logpdf_tv_sum(priorsM[i], θMs[:,i])
            #Tθ = Base.return_types(getindex, Tuple{typeof(θMs), Colon, typeof(i)})
            #TRET = Base.return_types(logpdf_tv_sum, Tuple{typeof(priorsM[i]), typeof(θMs[:,i])})
        end
    end
    nlMs_sum = sum(f_col, 1:length(priorsM), init = zero(nlP0)) #::typeof(nlP0) # not type inferred in julia 1.10
    neg_log_prior_i = nlP0 - nlMs_sum
    if !isfinite(neg_log_prior_i)
        @show neg_log_prior_i, nlP0
        @show θMs
        @show priorsM
        error("inspect non-finite priors")
    end
    neg_log_prior_i
end

"""
    zero_penalty_loss(y_pred, θMs, θP, ϕg, ϕq)

Add zero i.e. no additional loss terms during the HVI fit.

The basic cost in HVI is the negative log of the joint probability, i.e.
the likelihood of the observations given the parameters * prior probability
of the parameters.

Sometimes there is additional knowledge not encoded in the prior, such as
one parameter must be larger than another, or entropy-weights of the
ML-parameters, and the solver accept a function to add additional loss terms.

Arguments
- y_pred::AbstractMatrix: Observations
- θMs::AbstractMatrix: site parameters
- θP::AbstractVector: global parameters
- ϕg: ML-model parameters, 
- ϕq::AbstractVector, additional parameters of the posterior
"""
function zero_penalty_loss(
    y_pred::AbstractMatrix, θMs::AbstractMatrix, θP::AbstractVector, 
    ϕg, ϕq::AbstractVector)
    return zero(eltype(θMs))
end


"""
    predict_hvi([rng], prob::AbstractHybridProblem)

Prediction function for hybrid variational inference parameter model. 

## Arguments
- `prob`: The problem for which to predict

## Keyword arguments
- `scenario`
- `n_sample_pred`
- `xM`: covariates for the machine-learning model (ML): Matrix (n_θM x n_site_pred).
  Possibility to override the default from `get_hybridproblem_train_dataloader`.
- `xP`: model drivers for process based model (PBM): Matrix with (n_site_pred) rows.
  Possibility to override the default from `get_hybridproblem_train_dataloader`.

Returns an NamedTuple `(; y, θsP, θsMs, entropy_ζ)` with entries
- `y`: Array `(n_obs, n_site, n_sample_pred)` of model predictions.
- `θsP`: ComponentArray `(n_θP, n_sample_pred)` of PBM model parameters
  that are kept constant across sites.
- `θsMs`: ComponentArray `(n_site, n_θM, n_sample_pred)` of PBM model parameters
  that vary by site.
- `entropy_ζ`: The entropy of the log-determinant of the transformation of 
  the set of model parameters, which is involved in uncertainty quantification.
"""
function predict_hvi(rng, prob::AbstractHybridProblem; scenario=Val(()), 
    gdevs = get_gdev_MP(scenario), 
    xM = nothing, xP = nothing,
    is_testmode = true,
    kwargs...
    )
    if isnothing(xM) || isnothing(xP)
        dl = get_hybridproblem_train_dataloader(prob; scenario)
        dl_dev = gdev_hybridproblem_dataloader(dl; gdevs)
        xM_dl, xP_dl = dl_dev.data[1:2]
        xM = isnothing(xM) ? xM_dl : xM
        xP = isnothing(xP) ? xP_dl : xP
    end
    (; θsP, θsMs, entropy_ζ) = sample_posterior(
        rng, prob, xM; scenario, gdevs, is_testmode, kwargs...)
    #
    n_site, n_batch = get_hybridproblem_n_site_and_batch(prob; scenario)
    n_site_pred = size(θsMs,1) # determined by size(xM)
    @assert size(xP, 2) == n_site_pred
    f_batch = get_hybridproblem_PBmodel(prob; scenario)
    f = n_site_pred == n_batch ? f_batch : create_nsite_applicator(f_batch, n_site_pred)
    if gdevs.gdev_P isa MLDataDevices.AbstractGPUDevice
        f_dev = gdevs.gdev_P(f)#fmap(gdevs.gdev_P, f)
    else
        f_dev = f
    end
    #y = apply_process_model(θsP, θsMs, f_dev, xP)
    y = f_dev(θsP, θsMs, xP)
    (; y, θsP, θsMs, entropy_ζ)
end

"""
    sample_posterior(rng, prob, [xM::AbstractMatrix]; scenario=Val(()), kwargs...)

Sampling the posterior parameter distribution 
for hybrid variational inference problem. 

## Arguments
- `rng`: random number generator
- `prob`: The AbstractHybridProblem from to sample
- `xM`: covariates for the machine-learning model (ML): Matrix `(n_θM x n_site_pred)`.
  Default to all sites in `get_hybridproblem_train_dataloader(prob; scenario)`.

Optional keyword arguments    
- `scenario`: scenario to query `prob` and set default of gpu devices.
- `n_sample_pred`: number of samples to draw, defaults to 200
- `gdevs`: `NamedTuple(gdev_M, gdev_P)`: GPU devices for machine learning model 
  and parameter transformtation, default to [`get_gdev_MP`](@ref)`(scenario)`.
- `is_inferred`: set to `Val(true)` to activate type stabilicy check for transformation

Returns an NamedTuple `(; θsP, θsMs, entropy_ζ)` with entries
- `θsP`: ComponentArray `(n_θP, n_sample_pred)` of PBM model parameters
  that are kept constant across sites.
- `θsMs`: ComponentArray `(n_site, n_θM, n_sample_pred)` of PBM model parameters
  that vary by site.
- `entropy_ζ`: The entropy of the log-determinant of the transformation of 
  the set of model parameters, which is involved in uncertainty quantification.
"""
function sample_posterior(rng, prob::AbstractHybridProblem; scenario=Val(()), 
    gdevs = get_gdev_MP(scenario),
    kwargs...)
    dl = get_hybridproblem_train_dataloader(prob; scenario)
    dl_dev = gdev_hybridproblem_dataloader(dl; gdevs)
    xM = dl_dev.data[1]
    sample_posterior(rng, prob, xM; scenario, gdevs, kwargs...)
end


function sample_posterior(rng, prob::AbstractHybridProblem, xM::AbstractMatrix;
    scenario=Val(()),
    n_sample_pred=200,
    gdevs = get_gdev_MP(scenario),
    approx = nothing,
    kwargs...
)
    n_site, n_batch = get_hybridproblem_n_site_and_batch(prob; scenario)
    is_predict_batch = (n_batch == size(xM,2))
    n_site_pred = is_predict_batch ? n_batch : n_site
    @assert size(xM, 2) == n_site_pred
    par_templates = get_hybridproblem_par_templates(prob; scenario)
    cor_ends = get_hybridproblem_cor_ends(prob; scenario)
    g, ϕg0 = get_hybridproblem_MLapplicator(prob; scenario)
    ϕq = get_hybridproblem_ϕq(prob; scenario)
    (; transP, transM) = get_hybridproblem_transforms(prob; scenario)
    pbm_covars = get_hybridproblem_pbmpar_covars(prob; scenario)
    pbm_covar_indices = get_pbm_covar_indices(par_templates.θP, pbm_covars)
    (; ϕ, interpreters) = init_hybrid_params(ϕg0, ϕq)
    int_ϕg_ϕq = interpreters.ϕg_ϕq
    int_ϕq = interpreters.ϕq
    transMs = StackedArray(transM, n_batch)        
    g_dev, ϕ_dev = gdevs.gdev_M(g), gdevs.gdev_M(ϕ)
    if isnothing(approx) 
        approx = prob.approx  # assuming has field approx, e.g. if its a HybridProblem
    end
    (; θsP, θsMs, entropy_ζ) = sample_posterior(rng, g_dev, ϕ_dev, xM;
        int_ϕg_ϕq, int_ϕq, transP, transM, 
        n_sample_pred, cdev=infer_cdev(gdevs), cor_ends, pbm_covar_indices, approx,
        kwargs...)
    θsPc = ComponentArrayInterpreter(par_templates.θP, (n_sample_pred,))(θsP)
    θsMsc = ComponentArrayInterpreter((n_site,), par_templates.θM, (n_sample_pred,))(θsMs)
    (; θsP=θsPc, θsMs=θsMsc, entropy_ζ)
end

function sample_posterior(rng, g, ϕ::AbstractVector, xM::AbstractMatrix;
    int_ϕg_ϕq::AbstractComponentArrayInterpreter,
    int_ϕq::AbstractComponentArrayInterpreter,
    transP, transM,
    n_sample_pred,
    cdev,
    cor_ends,
    pbm_covar_indices,
    is_inferred::Val{is_infer} = Val(false),
    is_testmode,
    approx::AbstractHVIApproximation,
) where is_infer
    ζsP_gpu, ζsMs_gpu, σ = generate_ζ(approx, rng, g, CA.getdata(ϕ), CA.getdata(xM);
        int_ϕg_ϕq, int_ϕq,
        n_MC=n_sample_pred, cor_ends, pbm_covar_indices, is_testmode)
    ζsP = cdev(ζsP_gpu)
    ζsMs = cdev(ζsMs_gpu)
    logdetΣ = 2 * sum(log.(σ))
    entropy_ζ = entropy_MvNormal(length(σ), logdetΣ)
    trans_mP = StackedArray(transP, size(ζsP, 2))
    trans_mMs = StackedArray(transM, size(ζsMs, 1) * size(ζsMs, 3))
    θsP, θsMs = is_infer ? 
        Test.@inferred(transform_ζs(ζsP, ζsMs; trans_mP, trans_mMs)) :
        transform_ζs(ζsP, ζsMs; trans_mP, trans_mMs)
    (; θsP, θsMs, entropy_ζ)
end


"""
Generate samples of (inv-transformed) model parameters, ζ, 
and the vector of standard deviations, σ, i.e. the diagonal of the cholesky-factor.

Adds the MV-normally distributed residuals, retrieved by `sample_ζresid_norm`
to the means extracted from parameters and predicted by the machine learning
model. 

The output shape of size `(n_site x n_par x n_MC)` is tailored to iterating
each MC sample and then transforming each parameter on block across sites.
"""
function generate_ζ(approx::AbstractMeanHVIApproximation, rng::AbstractRNG, 
    g, ϕ::AbstractVector{FT}, xM::MT;
    int_ϕg_ϕq::AbstractComponentArrayInterpreter,
    int_ϕq::AbstractComponentArrayInterpreter,
    n_MC=3, cor_ends, pbm_covar_indices,
    is_testmode,
    ) where {FT,MT}
    # see documentation of neg_elbo_gtf
    ϕc = int_ϕg_ϕq(CA.getdata(ϕ))
    #VT= typeof(@view(ϕ[1:1]))
    ϕg = CA.getdata(ϕc[Val(:ϕg)])
    ϕq = CA.getdata(ϕc[Val(:ϕq)])
    ϕqc = int_ϕq(ϕq)
    μ_ζP = CA.getdata(ϕqc[Val(:μP)])
    # first pass: append μ_ζP_to covars, need ML prediction for magnitude of ζMs
    # TODO replace pbm_covar_indices by ComponentArray? dimensions to be type-inferred?
    xMP0 = _append_each_covars(xM, CA.getdata(μ_ζP), pbm_covar_indices)
    ϕm0 = g(xMP0, ϕg; is_testmode)
    μ_ζMs0 = ϕm0
    ζP_resids, ζMs_parfirst_resids, σ = sample_ζresid_norm(approx, rng, ϕm0, ϕq; n_MC, cor_ends, int_ϕq)
    ζsP = isempty(μ_ζP) ? ζP_resids : (μ_ζP .+ ζP_resids)  # n_par x n_MC 
    if pbm_covar_indices isa SA.SVector{0}
        # do not need to predict again but just add the residuals to μ_ζP and μ_ζMs
        #ζsP = μ_ζP .+ ζP_resids  # n_par x n_MC # .+ on empty view does not work
        ζsMs = permutedims(μ_ζMs0 .+ ζMs_parfirst_resids, (2, 1, 3)) # n_site x n_par x n_MC
        # if any(ζsMs[:,2,:] .> 80.0)
        #     @show ζsMs
        #     @show ζMs_parfirst_resids
        #     @show ϕc.ϕq.coef_logσ2_ζMs
        #     error("encountered scaled residual outside envisoned range. Debug") 
        # end
    else
        #rP, rMs = first(zip(eachcol(ζP_resids), eachslice(ζMs_parfirst_resids;dims=3)))
        ζsMs_vec = map(eachcol(ζsP), eachslice(ζMs_parfirst_resids; dims=3)) do ζP, rMs
            # second pass: append ζP rather than μ_ζP to covars to xM
            xMP = _append_each_covars(xM, CA.getdata(ζP), pbm_covar_indices)
            μ_ζMst = ϕm = g(xMP, ϕg; is_testmode)
            ζMs = (μ_ζMst .+ rMs)'  # already transform to par-last form
            ζMs
        end
        # ζsP = stack(map(first, ζst); dims=1)  # n_MC x n_par
        # ζsMs = stack(map(x -> x[2], ζst); dims=1) # n_MC x n_site x n_par
        ζsMs = stack(ζsMs_vec) # n_site x n_par x n_MC
    end
    ζsP, ζsMs, σ
end

# function _append_PBM_covars(xM, ζP, pbm_covars::NTuple{N,Symbol}) where N
#     #@show ζP, typeof(ζP)
#     vcat(xM, hcat(fill(
#         convert(Array{eltype(xM)}, CA.getdata(ζP[pbm_covars])),
#         #similar(CA.getdata(ζP[pbm_covars]), eltype(xM)), # 
#         size(xM,2))...))
# end
# function _append_PBM_covars(xM, ζP, pbm_covars::NTuple{0}) 
#     xM
# end

function _append_each_covars(xM, ζP::AbstractVector, pbm_covar_indices::SA.StaticVector{0})
    xM
end
function _append_each_covars(xM, ζP::AbstractVector, pbm_covar_indices::AbstractVector)
    ζP_covar = ζP[pbm_covar_indices]
    _append_each_covars(xM, ζP_covar)
end
function _append_each_covars(xM, ζP_covar::AbstractVector)
    #@show ζP, typeof(ζP)
    @assert eltype(xM) == eltype(ζP_covar)
    #Main.@infiltrate_main
    ζP_rep = reduce(hcat, fill(ζP_covar, size(xM, 2)))
    vcat(xM, ζP_rep)
end

function get_pbm_covar_indices(ζP, pbm_covars::NTuple{N,Symbol},
    intP::AbstractComponentArrayInterpreter=ComponentArrayInterpreter(ζP)) where {N}
    #SA.SVector{N}(CA.getdata(intP(1:length(intP))[pbm_covars])) # can not index into GPUarr
    CA.getdata(intP(1:length(intP))[pbm_covars])
end
function get_pbm_covar_indices(ζP, pbm_covars::NTuple{0},
    intP::AbstractComponentArrayInterpreter=ComponentArrayInterpreter(ζP))
    SA.SA[]
end

# remove?
# # function _predict_μ_ζMs(xM, ζP, pbm_covars::NTuple{N,Symbol}, g, ϕg, μ_ζMs0) where N
# #     xMP2 = _append_PBM_covars(xM, ζP, pbm_covars) # need different variable name?
# #     μ_ζMs = g(xMP2, ϕg)
# # end
# # function _predict_μ_ζMs(xM, ζP, pbm_covars::NTuple{0}, g, ϕg, μ_ζMs0)
# #     # if pbm_covars is the empty tuple, just return the original prediction on xM only
# #     # rather than calling the ML model
# #     μ_ζMs0
# # end

# function _predict_μ_ζMs(xM, ζP, pbm_covar_indices::AbstractVector, g, ϕg, μ_ζMs0)
#     xMP2 = _append_each_covars(xM, CA.getdata(ζP), pbm_covar_indices)
#     μ_ζMs = g(xMP2, ϕg)
# end
# function _predict_μ_ζMs(xM, ζP, pbm_covars_indices::SA.StaticVector{0}, g, ϕg, μ_ζMs0)
#     # if pbm_covars is the empty tuple, just return the original prediction on xM only
#     # rather than calling the ML model
#     μ_ζMs0
# end

"""
Extract relevant parameters from ζ and return n_MC generated multivariate normal draws
together with the vector of standard deviations, `σ`: `(ζP_resids, ζMs_parfirst_resids, σ)`
The output shape `(n_θ, n_site?, n_MC)` is tailored to adding `ζMs_parfirst_resids` to
ML-model predcitions of size `(n_θM, n_site)`.

## Arguments
* `int_ϕq`: Interpret vector as ComponentVector with components
   ρsP, ρsM, logσ2_ζP, coef_logσ2_ζMs(intercept + slope), 
"""
function sample_ζresid_norm(approx::AbstractHVIApproximation, rng::Random.AbstractRNG, 
    ϕm::AbstractMatrix, ϕq::AbstractVector,
    args...; 
    n_MC, cor_ends, int_ϕq)
    ζP = int_ϕq(CA.getdata(ϕq))[Val(:μP)]
    ζMs = ϕm
    n_θP, n_θMs = length(ζP), length(ζMs)
    # intm_PMs_parfirst = !isnothing(intm_PMs_parfirst) ? intm_PMs_parfirst : begin
    #     n_θM, n_site_batch = size(ζMs)
    #     get_concrete(ComponentArrayInterpreter(
    #         P = (n_MC, n_θP), Ms = (n_MC, n_θM, n_site_batch)))
    # end
    #urandn = _create_randn(rng, CA.getdata(ζP), n_MC, n_θP + n_θMs)
    #z = _create_randn(rng, CA.getdata(ζP), n_MC, n_θP)
    zP = _create_randn(rng, CA.getdata(ζP), n_MC, n_θP)
    zMs = _create_randn(rng, CA.getdata(ζP), n_MC, n_θMs)
    sample_ζresid_norm(approx, zP, zMs, CA.getdata(ϕm), ϕq, args...;
        cor_ends, 
        int_ϕq=get_concrete(int_ϕq)
    )
end

function sample_ζresid_norm(approx::MeanHVIApproximationMat, 
    zP::AbstractMatrix, zMs::AbstractMatrix, 
    ϕm::TM, ϕq::AbstractVector{T};
    int_ϕq=get_concrete(ComponentArrayInterpreter(ϕq)),
    cor_ends
) where {T,TM<:AbstractMatrix{T}}
    ζMs = ϕm
    ϕuncc = ϕqc = int_ϕq(CA.getdata(ϕq))
    ζP = ϕqc[Val(:μP)]
    n_θP, n_θMs, (n_θM, n_batch) = length(ζP), length(ζMs), size(ζMs)
    # do not create a UpperTriangular Matrix of an AbstractGÜUArray in transformU_cholesky1
    ρsP = isempty(ϕuncc[Val(:ρsP)]) ? similar(ϕuncc[Val(:ρsP)]) : ϕuncc[Val(:ρsP)] # required by zygote
    UP = transformU_block_cholesky1(ρsP, cor_ends.P)
    ρsM = isempty(ϕuncc[Val(:ρsM)]) ? similar(ϕuncc[Val(:ρsM)]) : ϕuncc[Val(:ρsM)] # required by zygote
    # cholesky factor of the correlation: diag(UM' * UM) .== 1
    # coefficients ρsM can be larger than 1, still yielding correlations <1 in UM' * UM
    UM = transformU_block_cholesky1(ρsM, cor_ends.M)
    cf = ϕuncc[Val(:coef_logσ2_ζMs)]
    logσ2_logMs = vec(cf[1, :] .+ cf[2, :] .* ζMs)
    logσ2_ζP = vec(CA.getdata(ϕuncc[Val(:logσ2_ζP)]))
    # CUDA cannot multiply BlockDiagonal * Diagonal, construct already those blocks
    σMs = reshape(exp.(logσ2_logMs ./ 2), n_θM, :)
    σP = exp.(logσ2_ζP ./ 2)
    # BlockDiagonal does work with CUDA, but not with combination of Zygote and CUDA
    # need to construct full matrix for CUDA
    Uσ, diagUσ = _compute_choleskyfactor(UP, UM, σP, σMs, n_batch) # inferred only BlockDiagonal
    #diagUσ = diag(Uσ)::typeof(σP)   # elements of the diagonal: standard deviations
    n_MC = size(zP, 1) 
    # is this multiplication efficient if Uσ is not concrete but only sumtype BlockDiagonal?
    urandn = hcat(zP, zMs)
    ζ_resids_parfirst = (Uσ' * urandn') #::typeof(urandn) # n_par x n_MC
    #ζ_resids_parfirst = (urandn * Uσ)' #::typeof(urandn) # n_par x n_MC
    #ζ_resids_parfirst = urandn' * Uσ # n_MC x n_par
    # need to handle empty(ζP) explicitly, otherwise Zygote tries to take gradient
    ζP_resids = isempty(ζP) ? ζ_resids_parfirst[1:0, :] : ζ_resids_parfirst[1:n_θP, :]
    ζMs_parfirst_resids = reshape(ζ_resids_parfirst[(n_θP+1):end, :], n_θM, n_batch, n_MC)
    ζP_resids, ζMs_parfirst_resids, diagUσ
    # #map(std, eachcol(ζ_resids_parfirst[:, 3:8]))
    # ζ_resid = transpose_mPMs_sitefirst(ζ_resids_parfirst; intm_PMs_parfirst)
    # #map(std, eachcol(ζ_resid[:, 3:8])) # all ~ 0.1 in sample_ζresid_norm cpu
    # #map(std, eachcol(ζ_resid[:, 2 + n_batch .+ (-1:5)])) # all ~ 100, except first two
    # # returns AbstractGPUuArrays to either continue on GPU or need to transfer to CPU
    # ζ_resid, diagUσ
end

"""
Transforms each row of a matrix (n_MC x n_Par) with site parameters Ms inside n_Par 
of form (n_par x n_site) to Ms of the form (n_site x n_par), i.e. 
neighboring entries (inside a column) are of the same parameter.

This format of having n_par as the last dimension helps transforming parameters
on block.
"""
function transpose_mPMs_sitefirst(Xt, n_θP::Integer, n_θM, n_site_batch, n_MC)
    # cannot make n_θP keyword arguments, because it overrides method below
    intm_PMs_parfirst = ComponentArrayInterpreter(
        P=(n_MC, n_θP), Ms=(n_MC, n_θM, n_site_batch))
    transpose_mPMs_sitefirst(Xt; intm_PMs_parfirst)
end
function transpose_mPMs_sitefirst(Xt;
    intm_PMs_parfirst=ComponentArrayInterpreter(
        P=(n_MC, n_θP), Ms=(n_MC, n_θM, n_site_batch))
)
    Xtc = intm_PMs_parfirst(Xt)
    # Main.@infiltrate_main

    # _Ms = Xtc.Ms
    # map(std, eachrow(_Ms[:,1:6,:]))

    # map(std, eachrow(tmp[3:8,:]))
    # _Ms = permutedims(Xtc.Ms, (1, 3, 2))
    #Main.@infiltrate_main
    #tmp = Test.@inferred getindex(Xtc, Val(:Ms))
    #tmp2= Test.@inferred getindex(Xtc, Val(:P))
    X_site_first = CA.ComponentVector(P=Xtc.P, Ms=permutedims(Xtc.Ms, (1, 3, 2)))
    reshape(CA.getdata(X_site_first), size(Xt))::typeof(CA.getdata(Xt))
    # X_site_first = CA.ComponentVector(
    #     P = permutedims(Xtc.P), Ms = permutedims(Xtc.Ms, (3, 2, 1)))
    # reshape(CA.getdata(X_site_first), rev(size(Xt)))::typeof(CA.getdata(Xt))
end

"""
Create Cholesky-factor of Covariance matrix.
from Cholesky-factors of correlation matrices (UP, UM) and diagonal sqrt(variances) (σP, σMs)
Return this factor and its diagonal
"""
function _compute_choleskyfactor(UP::AbstractMatrix{T}, UM, σP, σMs, n_batch) where {T}
    # v = if isempty(UP) 
    #   [UM * Diagonal(σMs[:, i]) for i in 1:n_batch] #::Vector{<:AbstractMatrix{T}}
    # else
    #   [i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch] #::Vector{<:AbstractMatrix{T}}
    # end
    Uσ_blocks = [i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch] 
    # vcat not type stable
    #Uσ_blocks = vcat([UP * Diagonal(σP)], [UM * Diagonal(σMs[:, i]) for i in 1:n_batch]) 
    Uσ = BlockDiagonal(Uσ_blocks)
    # diag(Uσ) is a performance bottleneck, therefore construct explicitly
    #diagUσ = [i == 0 ? diag(UP) .* σP : diag(UM) .* σMs[:, i] for i in 0:n_batch]
    # not type stable diagUσM = reduce(vcat, Tuple(diag(UM) .* σMs[:, i] for i in 1:n_batch); init = σP[1:0])
    #diagUσM = reduce(vcat, [diag(UM) .* σMs[:, i] for i in 1:n_batch]; init = σMs[1:0])
    empty_arr = ChainRulesCore.@ignore_derivatives eltype(σMs)[]
    #diagUσM = reduce(vcat, [diag(UM) .* σMs[:, i] for i in 1:n_batch]; init=empty_arr)
    #diag U = ones -> diag Uσ = σ
    # diagUσM = vcat([diag(UM) .* σMs[:, i] for i in 1:n_batch]...)::Vector{T}
    # diagUσ = vcat(diag(UP) .* σP, diagUσM)
    diagUσ = vcat(σP, vec(σMs))
    # diagUσ == diag(Uσ)
    Uσ, diagUσ
end
function _compute_choleskyfactor(
    UP::GPUArraysCore.AbstractGPUMatrix{T}, UM, σP, σMs, n_batch) where {T}
    # using BlockDiagonal leads to Scalar operations downstream
    # v = [i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch]
    # BlockDiagonal(v)    
    # Uσ = cat([i == 0 ? UP * Diagonal(σP) : UM * Diagonal(σMs[:, i]) for i in 0:n_batch]...;
    #     dims=(1, 2))
    # on GPU use only one big multiplication rather than many small ones
    # U = if isempty(UP) 
    #   cat([UM for i in 1:n_batch]...; dims=(1, 2))
    # else
    #   cat([i == 0 ? UP : UM for i in 0:n_batch]...; dims=(1, 2))
    # end
    #U = cat([i == 0 ? UP : UM for i in 0:n_batch]...; dims=(1, 2))
    #U = cat(UP,[UM for i in 1:n_batch]...; dims=(1, 2)) # Zygote not work with generator
    U = cat(UP,Tuple(UM for i in 1:n_batch)...; dims=(1, 2)) # Zygote not work with generator
    #Main.@infiltrate_main
    σD = Diagonal(vcat(σP, vec(σMs)))
    Uσ = U * σD
    # need for Zygote why?
    B = vcat(Uσ)
    diagUσ = diag(B)
    B, diagUσ
end

# TODO replace by KA.rand when it becomes available, see ones_similar
# https://github.com/JuliaGPU/KernelAbstractions.jl/issues/488
function _create_randn(rng, ::AbstractVector{T}, dims...) where {T}
    randn(rng, T, dims...)
end
#moved to HybridVariationalInferenceCUDAExt
#function _create_randn(rng, ::CUDA.CuVector{T}, dims...) where {T}

"""
Transform parameters and compute absolute of determinant of Jacobian of the transformation.
- from unconstrained (e.g. log) ζ scale of format (n_site x n_par)
- to constrained θ scale of format (n_site x n_par)
"""

function transform_and_logjac_ζ(ζP::AbstractVector, ζMs::AbstractMatrix;
    transP::Bijectors.Transform,
    transMs::StackedArray=StackedArray(transM, size(ζMs, 1)))
    θMs, logjac_M = Bijectors.with_logabsdet_jacobian(transMs, ζMs)
    # if !all(isfinite.(θMs)) 
    #     @show θMs
    #     @show ζMs
    #     @show transMs
    #     warning("encountered non-finite θMs. Debug the inversion. " *
    #     "This may happen for example if transforming a large value in sampling space by "*
    #     "exponential to parameter space. You may use a RangeScalingChian instead.")
    # end   
    # constrain parameters to sqrt of range of numeric type
    # due to sampling they might exceed the range
    θMs = min.(sqrt(floatmax(eltype(θMs))), θMs) 
    θMs = max.(sqrt(floatmin(eltype(θMs))), θMs) 
    θP, logjac_P = if isempty(ζP)
        ζP, zero(logjac_M)  
    else
        θP, logjac_P = Bijectors.with_logabsdet_jacobian(transP, ζP)
        θP = min.(sqrt(floatmax(eltype(θP))), θP) 
        θP = max.(sqrt(floatmin(eltype(θP))), θP) 
        θP, logjac_P
    end
    θP, θMs, logjac_P + logjac_M
end

"""
Transform parameters 
- from unconstrained (e.g. log) ζ scale of format ((n_site x n_par) x n_mc)
- to constrained θ scale of the same format
"""
function transform_ζs(ζsP::AbstractMatrix, ζsMs::AbstractArray;
    trans_mP::StackedArray=StackedArray(transP, n_MC),
    trans_mMs::StackedArray=StackedArray(transM, n_MC * n_site_batch)
)
    # transform to parameter-last that can apply transformations effectively
    θsMst = trans_mMs(permutedims(ζsMs, (3, 1, 2)))
    # backtransform to n_mc last for efficient mapping
    θsMs = permutedims(θsMst, (2, 3, 1))
    θsPt = trans_mP(ζsP') # Bijectors use copy and copy(ζsP') errors if ζsP is an empty CuArray
    θsP = θsPt'
    # θsP = if isempty(ζsP) 
    #     # trans_mP(ζsP') of empty array has problems with AD
    #     # copy(ζsP')'  # copy of empty array does no harm but ensures type is Adjoint
    #     ζsP  # leads to type instability
    #     #θsMs[1, 1:0, :] # workaround: extract empty matrix from first mc_batch of θsMs not the same type
    # else
    #     θsPt = trans_mP(ζsP')
    #     θsP = θsPt'
    # end
    θsP, θsMs
end

function flatten_hybrid_pars(xsP::AbstractMatrix{FT}, xsMs::AbstractArray{FT,3}) where FT
    n_site_pred, n_θM, n_MC = size(xsMs)
    @assert size(xsP,2) == n_MC
    vcat(xsP, reshape(xsMs, n_site_pred * n_θM, n_MC))
end







